# Meta-Learning Architecture Flow

  
`meta_learn.py` ìŠ¤í¬ë¦½íŠ¸ì˜ ì•„í‚¤í…ì²˜ì™€ ì‹¤í–‰ íë¦„ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---
## 1. ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

  

```mermaid

flowchart TB

subgraph Input["ğŸ”¹ ì…ë ¥ ë‹¨ê³„"]

A[CelebA Dataset<br/>ì–¼êµ´ ì´ë¯¸ì§€ ë°ì´í„°ì…‹] --> B[Image Preprocessing<br/>64x64 Grayscale ë³€í™˜]

C[SIREN Model<br/>w0=30, width=256, depth=5] --> D[Initial Parameters<br/>ëœë¤ ì´ˆê¸°í™”]

end

  

subgraph Meta["ğŸ”¸ Meta-Learning Loop"]

E[Outer Loop<br/>5000 iterations] --> F[Inner Loop<br/>2 steps per image]

F --> G[Parameter Update<br/>MAML or REPTILE]

G --> E

end

  

subgraph Output["ğŸ”¹ ì¶œë ¥"]

H[Meta-Learned Parameters<br/>maml_celebA_5000.pickle]

end

  

B --> Meta

D --> Meta

Meta --> H

```

  

---
## 2. SIREN ëª¨ë¸ êµ¬ì¡°

SIREN (Sinusoidal Representation Network)ì€ ì¢Œí‘œ `(x, y)`ë¥¼ ì…ë ¥ë°›ì•„ í”½ì…€ ê°’ì„ ì¶œë ¥í•˜ëŠ” INRì…ë‹ˆë‹¤.
  
```mermaid

flowchart LR

subgraph SIREN["SIREN Architecture"]

I["Input<br/>(x, y) âˆˆ [0,1]Â²"] --> L1["Linear Layer<br/>2 â†’ 256"]

L1 --> S1["sin(Ï‰â‚€ Â· x)<br/>Ï‰â‚€ = 30"]

S1 --> L2["Linear Layer<br/>256 â†’ 256"]

L2 --> S2["sin(Ï‰ Â· x)<br/>Ï‰ = 30"]

S2 --> L3["...Ã—3 ë” ë°˜ë³µ"]

L3 --> L4["Linear Layer<br/>256 â†’ 1"]

L4 --> O["Output<br/>Pixel Value"]

end

  

style I fill:#e1f5fe

style O fill:#e8f5e9

```

  
### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„°        | ê°’   | ì„¤ëª…                |
| ----------- | --- | ----------------- |
| `w0`        | 30  | ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ |
| `hidden_w0` | 30  | íˆë“  ë ˆì´ì–´ë“¤ì˜ ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼  |
| `width`     | 256 | ê° ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜       |
| `depth`     | 5   | ì´ ë ˆì´ì–´ ìˆ˜           |
 
---
## 3. Meta-Learning ì•Œê³ ë¦¬ì¦˜ íë¦„

  
### 3.1 MAML (Model-Agnostic Meta-Learning) ë°©ì‹
  

```mermaid

flowchart TB

subgraph Outer["Outer Loop (Meta Update)"]

direction TB

P0["Î¸: Meta Parameters"] --> BATCH["Sample Batch<br/>3 images"]

  

subgraph Inner["Inner Loop (Task Adaptation)"]

direction LR

IMG1["Image 1"] --> ADAPT1["Î¸ â†’ Î¸'â‚<br/>2 SGD steps"]

IMG2["Image 2"] --> ADAPT2["Î¸ â†’ Î¸'â‚‚<br/>2 SGD steps"]

IMG3["Image 3"] --> ADAPT3["Î¸ â†’ Î¸'â‚ƒ<br/>2 SGD steps"]

end

  

BATCH --> Inner

  

ADAPT1 --> LOSS["Loss = Î£ MSE(f_Î¸'áµ¢, Imageáµ¢)"]

ADAPT2 --> LOSS

ADAPT3 --> LOSS

  

LOSS --> GRAD["âˆ‡_Î¸ Loss<br/>Backprop through Inner Loop"]

GRAD --> UPDATE["Î¸ â† Î¸ - Î±Â·âˆ‡_Î¸ Loss<br/>Adam optimizer"]

UPDATE --> P0

end

  

style P0 fill:#fff3e0

style UPDATE fill:#e8f5e9

```

  

### 3.2 Inner Loop vs Outer Loop

  

```mermaid

sequenceDiagram

participant Î¸ as Meta Params (Î¸)

participant Inner as Inner Loop

participant Outer as Outer Loop

  

Note over Î¸: ì´ˆê¸° ëœë¤ íŒŒë¼ë¯¸í„°

  

loop 5000 iterations

Î¸->>Inner: Copy parameters

  

loop 2 inner steps (per image)

Inner->>Inner: loss = MSE(model(coords), image)

Inner->>Inner: Î¸' â† Î¸' - 0.01Â·âˆ‡loss (SGD)

end

  

Inner->>Outer: Adapted params Î¸'

Outer->>Outer: Meta-loss = MSE(f_Î¸', image)

Outer->>Î¸: Î¸ â† Î¸ - 1e-5Â·âˆ‡meta_loss (Adam)

end

  

Note over Î¸: Meta-learned params ì €ì¥

```

  

---
## 4. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
  

```mermaid

flowchart LR

subgraph Dataset["CelebA Dataset"]

RAW["Raw Image<br/>218Ã—178 RGB"]

end

  

subgraph Process["Preprocessing"]

CROP["Center Crop<br/>178Ã—178"]

GRAY["Grayscale<br/>ë³€í™˜"]

RESIZE["Resize<br/>64Ã—64"]

end

  

subgraph Coord["Coordinate Grid"]

GRID["(x,y) Grid<br/>64Ã—64 = 4096 points<br/>x,y âˆˆ [0,1]"]

end

  

RAW --> CROP --> GRAY --> RESIZE

RESIZE --> TARGET["Target Pixels<br/>4096Ã—1"]

GRID --> INPUT["Input Coords<br/>4096Ã—2"]

  

INPUT --> MODEL["SIREN"]

TARGET --> LOSS["MSE Loss"]

MODEL --> PRED["Predictions<br/>4096Ã—1"]

PRED --> LOSS

  

style TARGET fill:#ffebee

style PRED fill:#e8f5e9

```

  

---
## 5. í•µì‹¬ ê°œë…: Dictionary Learning ê´€ì 
  
ë…¼ë¬¸ì˜ í•µì‹¬ í†µì°°ì€ **Meta-Learningì´ Dictionary Learningê³¼ ìœ ì‚¬í•˜ë‹¤**ëŠ” ê²ƒì…ë‹ˆë‹¤.
  

```mermaid

flowchart TB

subgraph Before["ğŸ”´ Meta-Learning ì „"]

NTK1["NTK Eigenfunctions<br/>(Random patterns)"]

IMG1["Target Image"]

ENC1["âŒ ë¹„íš¨ìœ¨ì  ì¸ì½”ë”©<br/>ë§ì€ eigenfunction í•„ìš”"]

end

  

subgraph After["ğŸŸ¢ Meta-Learning í›„"]

NTK2["NTK Eigenfunctions<br/>(Face-like patterns)"]

IMG2["Target Image"]

ENC2["âœ… íš¨ìœ¨ì  ì¸ì½”ë”©<br/>ì ì€ eigenfunctionìœ¼ë¡œ í‘œí˜„"]

end

  

NTK1 --> ENC1

IMG1 --> ENC1

  

NTK2 --> ENC2

IMG2 --> ENC2

  

Before -->|"MAML Training<br/>on CelebA"| After

```

  

### ë…¼ë¬¸ ì¸ìš© (Section 5.3)

> "Meta-learning has a reshaping effect on the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training."
  

**í•´ì„**: MAMLë¡œ í•™ìŠµí•˜ë©´ NTKì˜ eigenfunctionsì´ ì–¼êµ´ ëª¨ì–‘ìœ¼ë¡œ reshape ë©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´:

- ìƒˆë¡œìš´ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ë” ë¹ ë¥´ê²Œ í•™ìŠµ
- ë” ì ì€ gradient stepìœ¼ë¡œ ìˆ˜ë ´
- ë” ì¢‹ì€ ì¼ë°˜í™” ì„±ëŠ¥

  

---
## 6. ì‹¤í–‰ íë¦„ ìƒì„¸
  
```mermaid

stateDiagram-v2

[*] --> Initialize: ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘

  

Initialize --> LoadData: SIREN ëª¨ë¸ ì´ˆê¸°í™”

LoadData --> MetaTrain: CelebA ë°ì´í„° ë¡œë“œ

  

state MetaTrain {

[*] --> SampleBatch

SampleBatch --> InnerLoop: 3ê°œ ì´ë¯¸ì§€ ìƒ˜í”Œë§

  

state InnerLoop {

[*] --> Forward

Forward --> ComputeLoss: ì¢Œí‘œ â†’ í”½ì…€ ì˜ˆì¸¡

ComputeLoss --> Backward: MSE ê³„ì‚°

Backward --> UpdateInner: Gradient ê³„ì‚°

UpdateInner --> CheckSteps: SGD ì—…ë°ì´íŠ¸

CheckSteps --> Forward: steps < 2

CheckSteps --> [*]: steps = 2

}

  

InnerLoop --> OuterUpdate: ì ì‘ëœ íŒŒë¼ë¯¸í„°

OuterUpdate --> Validate: Meta gradientë¡œ ì—…ë°ì´íŠ¸

Validate --> SampleBatch: iter < 5000

Validate --> [*]: iter = 5000

}

  

MetaTrain --> SaveParams: í•™ìŠµ ì™„ë£Œ

SaveParams --> [*]: pickle íŒŒì¼ ì €ì¥

```

  
---
## 7. ì½”ë“œ-ê°œë… ë§¤í•‘
  
| ì½”ë“œ ìœ„ì¹˜                       | ê°œë…               | ì„¤ëª…                       |
| --------------------------- | ---------------- | ------------------------ |
| `meta_learn.py:20-23`       | Model Init       | SIREN ëª¨ë¸ ìƒì„± ë° íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”   |
| `meta_learn.py:25-33`       | Data Load        | CelebA train/val ë°ì´í„°ì…‹ ì¤€ë¹„ |
| `train/meta_learn.py:23-38` | Inner Loop       | íƒœìŠ¤í¬ë³„ ì ì‘ (2 SGD steps)    |
| `train/meta_learn.py:42-71` | Outer Loop       | ë©”íƒ€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸             |
| `train/meta_learn.py:57-67` | MAML Gradient    | ë‚´ë¶€ ë£¨í”„ë¥¼ í†µí•œ ì—­ì „íŒŒ            |
| `train/meta_learn.py:44-54` | REPTILE Gradient | íŒŒë¼ë¯¸í„° ì°¨ì´ ê¸°ë°˜ ì—…ë°ì´íŠ¸          |
| `meta_learn.py:49-50`       | Save Output      | í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì €ì¥              |
 
---

## 8. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš”ì•½

```mermaid
flowchart TB
    subgraph HP["Meta-Learn Hyperparams"]
        subgraph TR["Training"]
            TR1["BATCH_SIZE: 3"]
            TR2["MAX_ITERS: 5000"]
            TR3["META_METHOD: MAML"]
        end
        subgraph IN["Inner Loop"]
            IN1["INNER_LR: 0.01"]
            IN2["INNER_STEPS: 2"]
            IN3["Optimizer: SGD"]
        end
        subgraph OU["Outer Loop"]
            OU1["OUTER_LR: 1e-5"]
            OU2["Optimizer: Adam"]
        end
        subgraph MO["Model"]
            MO1["w0: 30"]
            MO2["width: 256"]
            MO3["depth: 5"]
        end
        subgraph DA["Data"]
            DA1["Resolution: 64x64"]
            DA2["Grayscale: Yes"]
            DA3["Val Examples: 5"]
        end
    end

    style TR fill:#e3f2fd
    style IN fill:#fff3e0
    style OU fill:#e8f5e9
    style MO fill:#fce4ec
    style DA fill:#f3e5f5
```

  
---
## 9. ì¶œë ¥ íŒŒì¼ í™œìš©

`maml_celebA_5000.pickle`ì— ì €ì¥ëœ meta-learned parametersëŠ” ë‹¤ë¥¸ ì‹¤í—˜ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤:
  

```mermaid

flowchart LR

META["meta_learn.py"] -->|ìƒì„±| PICKLE["maml_celebA_5000.pickle"]

PICKLE -->|ë¡œë“œ| FIG4["figure_4.py<br/>NTK Eigenfunction ë¶„ì„"]

PICKLE -->|ë¡œë“œ| FIG5["figure_5.py<br/>Energy Concentration ë¶„ì„"]

  

style PICKLE fill:#fff9c4

```

  
ì´ íŒŒë¼ë¯¸í„°ë¡œ ì´ˆê¸°í™”ëœ SIRENì€:

1. **ë¹ ë¥¸ ìˆ˜ë ´**: ì ì€ gradient stepìœ¼ë¡œ ìƒˆ ì´ë¯¸ì§€ í•™ìŠµ
2. **NTK ë³€í˜•**: Eigenfunctionsì´ ì–¼êµ´ í˜•íƒœë¡œ reshape
3. **íš¨ìœ¨ì  ì¸ì½”ë”©**: ì ì€ eigenfunctionìœ¼ë¡œ ì–¼êµ´ ì‹ í˜¸ í‘œí˜„

  

---
## ì°¸ê³  ë¬¸í—Œ

- Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" (MAML)
- Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions" (SIREN)
- Tancik et al., "Meta-learned Neural Neural Representations" (Meta-SDF)