{
  "Page 1": {
    "1": "A Structured Dictionary Perspective on Implicit Neural Representations",
    "2": "Gizem Yüce*\n ETH Zurich\n gyuece@ethz.ch",
    "3": "Guillermo Ortiz-Jiménez Beril Besbinar Pascal Frossard\n École Polytechnique Fédérale de Lausanne (EPFL)\n {name.surname}@epfl.ch",
    "4": "Abstract",
    "5": "Implicit neural representations (INRs) have recently emerged as a promising alternative to classical discretized representations of signals.",
    "6": "Nevertheless, despite their practical success, we still do not understand how INRs represent signals.",
    "7": "We propose a novel unified perspective to theoretically analyse INRs.",
    "8": "Leveraging results from harmonic analysis and deep learning theory, we show that most INR families are analogous to structured signal dictionaries whose atoms are integer harmonics of the set of initial mapping frequencies.",
    "9": "This structure allows INRs to express signals with an exponentially increasing frequency support using a number of parameters that only grows linearly with depth.",
    "10": "We also explore the inductive bias of INRs exploiting recent results about the empirical neural tangent kernel (NTK).",
    "11": "Specifically, we show that the eigenfunctions of the NTK can be seen as dictionary atoms whose inner product with the target signal determines the final performance of their reconstruction.",
    "12": "In this regard, we reveal that meta-learning has a reshaping effect on the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training.",
    "13": "Our results permit to design and tune novel INR architectures, but can also be of interest for the wider deep learning theory community.",
    "14": "1. Introduction",
    "15": "Implicit neural representations (INRs) have recently emerged as a powerful alternative to classical, discretized, representations of multimedia signals [10, 14,33,34,43,51, 52,56,57].",
    "16": "In contrast to traditional methods, INRs parameterize the continuous mapping between coordinates and signal values using neural networks.",
    "17": "This allows for an efficient and compact representation of signals that can be easily integrated into modern differentiable learning pipelines.",
    "18": "The recent success of INRs in many applications, such as surface representation [51], volume rendering [30,34,44, *The first two authors contributed equally to this work.",
    "19": "G. Yüce did this work during an internship at EPFL.",
    "20": "Implicit Neural Representations",
    "21": "wwwwww",
    "22": "Input coordinates",
    "23": "ww",
    "24": "NTK eigenfunctions",
    "25": "Signal Dictionary",
    "26": "+",
    "27": "+",
    "28": "Figure 1. Conceptual illustration of our main theoretical contributions: i) Each layer of an INR increases the frequency support of the representation by splitting a signal into higher order harmonics.",
    "29": "ii) INRs can be interpreted as signal dictionaries whose atoms are the eigenfunctions of their NTK at initialization.",
    "30": "54] or generative modelling [8, 15] can be largely attributed to the development of new periodic representations that can circumvent the spectral bias of standard neural networks.",
    "31": "Indeed, there is ample evidence that the use of periodic representations [24,34,52,56] can mitigate the bias of standard architectures towards low frequency [47].",
    "32": "Nevertheless, even if INRs have become widely adopted in practice, the theoretical understanding of their principles and properties is rather limited.",
    "33": "For example, there is no clear criterion to select between different INR families, their parameters are mostly based on heuristics, and their limitations are not well understood.",
    "34": "These shortcomings are slowing down further research developments. In this work, we therefore take a step back and focus on understanding the mechanisms behind the success of modern INRs, but also their failure modes, in order to develop more informed design strategies.",
    "35": "We provide a unified perspective with the aim to answer the following important questions:",
    "36": "1. What is the expressive power of INRS?",
    "37": "1"
  },
  "Page 2": {
    "38": "2. How does initialization affect their inductive bias?",
    "39": "Specifically, we first leverage results from harmonic analysis and deep learning theory, and we discover that the expressive power of most INRs is equivalent to that of a structured signal dictionary whose atoms are integer harmonics of the frequencies that define their initial input mapping (see Fig. 1).",
    "40": "This unifies many INR architectures under a single perspective, and can serve to understand them better and mitigate some of their common problems.",
    "41": "Then, we delve deeper on the inductive bias of INRs.",
    "42": "We build upon the foundational work in [56], and exploit recent results in deep learning theory to give a new unifying framework to analyse the inductive bias of any INR architecture in terms of its empirical neural tangent kernel (NTK) [21].",
    "43": "In particular, we reveal the existence of a close analogy between the eigenfunctions of the empirical NTK and the atoms of a signal dictionary, and show that the difficulty of learning a signal with an INR is intimately connected to how efficiently it can be encoded by this dictionary.",
    "44": "Finally, we use our novel perspective to explain the role of meta-learning in improving the performance of INRs.",
    "45": "INRs are known to be notoriously inefficient, requiring long training times, and a large sample exposure to achieve good results, especially in 3D settings [18, 19,49].",
    "46": "However, recent works have shown that using meta-learning algorithms to initialize INRs can greatly improve their speed of convergence and sample complexity [50,55].",
    "47": "In this work, we show that meta-learning works as a dictionary learning algorithm, transforming the NTK of an INR into a rich signal dictionary whose atoms are formed by combinations of the examples seen during meta-training.",
    "48": "This increases the representation efficiency of the target signals by the NTK [41], thus improving performance and training speed.",
    "49": "In summary, the main contributions of our work are:",
    "50": "• We provide a unified perspective to theoretically analyze the expressive power and inductive bias of INRs.",
    "51": "We show that the frequency support of INRs grows exponentially with depth, as each layer splits its input into higher order harmonics, demonstrating their efficiency in representing wide spectrum signals.",
    "52": "We use this theory to explain the main failure modes of INRs: imperfect recovery and aliasing.",
    "53": "We show that the inductive bias of INRs can be characterized by the ability of their empirical NTKs to encode different target signals efficiently.",
    "54": "Finally, we discover that meta-learning greatly increases the encoding efficiency of the NTK by constructing a rich signal dictionary using different combinations of the meta-training tasks.",
    "55": "Overall, we believe that our findings can impact the future research in INRs and their applications, and contribute",
    "56": "to speeding up the development of new principled algorithms in the field.",
    "57": "It gives a fresh perspective to understand and alleviate the drawbacks of the current architectures, as well as new intuitions to design better INR algorithms.",
    "58": "Finally, our analysis on the effect of meta-learning can also be of broader interest for the deep learning theory community¹.",
    "59": "2. Implicit Neural Representations",
    "60": "The goal of an implicit neural representation is to encode a continuous target signal $g:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{C}$ using a neural network $f_{\\theta}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{C}$, parameterized by a set of weights $\\theta\\in\\mathbb{R}^{N}$, by representing the mapping between input coordinates $r\\in\\mathbb{R}^{D}$, e.g.. pixels, and signal values $g(r)\\in\\mathbb{R}^{C}$ e.g., RGB colors.",
    "61": "This is achieved minimizing a distortion measure, like mean-squared error, during training using some form of (stochastic) gradient descent (SGD).",
    "62": "The continuous parameterization of INRs allows to store signals at a constant memory cost regardless of the spatial resolution, which makes INRs standout for reconstructing high-dimensional signals, such as videos or 3D scenes.",
    "63": "The main challenge for INRs, though, is to reconstruct high frequency details present in most multimedia signals, e.g., textures in images.",
    "64": "Classical neural network architectures are well-known for their strong spectral bias towards lower frequencies [47], and this has made them traditionally useless for implicit representation tasks.",
    "65": "Recently, however, few works [51, 56] have come up with different solutions to circumvent the spectral bias of neural networks, allowing faster convergence and greater fidelity of INRs.",
    "66": "In what follows, we provide an overview of the main solutions under a unified architecture formulation.",
    "67": "Specifically, we note that most INR architectures can be decomposed into a mapping function $\\gamma:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{T}$ followed by a multilayer perceptron (MLP), with weights $W^{(l)}\\in\\mathbb{R}^{F_{l-1}\\times F_{l}}$ bias $b^{(l)}\\in\\mathbb{R}^{F_{l}}$, and activation function $\\rho^{(l)}:\\mathbb{R}\\rightarrow\\mathbb{R}$, applied elementwise;",
    "68": "at each layer $l=1,...,L-1.$ That is, if we denote by $z^{(l)}$ each layers post activation, most INR architectures compute",
    "69": "$z^{(0)}=\\gamma(r),$",
    "70": "(1)",
    "71": "$z^{(l)} = \\rho^{(l)} (W^{(l)} z^{(l-1)}+b^{(l)})$, $l=1,...,L-1$ $f_{\\theta}(r)=W^{(L)}z^{(L-1)}+b^{(L)}.$",
    "72": "We now examine the two most popular INR architectures: Fourier feature networks (FFNs) In [56], Tancik et al.",
    "73": "proposed to use a Fourier mapping $\\gamma(r)=sin(\\Omega r+\\phi),$ with parameters $\\Omega\\in\\mathbb{R}^{T\\times D}$ and $\\phi\\in\\mathbb{R}^{T}$ followed by an MLP with $\\rho^{(l)}=$ ReLU.",
    "74": "Specifically, they showed that initializing $\\Omega_{i,j}\\sim\\mathcal{N}(0,\\sigma^{2})$ with random Fourier features [48] can modulate the spectral bias of an FFN, with",
    "75": "Code to reproduce this work: github.com/gortizji/inr.dictionaries",
    "76": "2"
  },
  "Page 3": {
    "77": "larger values of $\\sigma$ biasing these networks towards higher frequencies.",
    "78": "Alternative formulations with deterministic initialization, commonly used for neural rendering algorithms [34] can be considered as a special category of these networks where the frequencies in $\\Omega$ are taken to be powers of 2 and the frequencies in $\\phi$ alternate between $\\{0,\\pi/2\\}$.",
    "79": "Sinusoidal representation networks (SIRENS) In [51], Sitzmann et al. proposed to use MLP with sinusoidal activations, i.e., $\\rho^{(l)}=$ sin, where the first layer post activation, $z^{(0)}=sin(\\omega_{0}(W^{(0)}r+b^{(0)}))$ can be interpreted as $\\gamma(r)=sin(\\Omega r+\\phi)$.",
    "80": "They showed that, by rescaling the parameters at initialization by the constant factor $\\omega_{0},$ SIRENs can also modulate the spectral bias, with larger $\\omega_{0}$ biasing these networks towards higher frequencies.",
    "81": "Nonetheless, despite the ample empirical evidence that shows that these architectures are effective at representing natural images or other visual signals, there is little theoretical understanding of how they do so.",
    "82": "Moreover, since the design of each of these networks is guided by very different principles, the sheer diversity in the structure of these architectures makes their analysis very involved.",
    "83": "In the next sections, we provide a unified perspective to analyze the expressive power and inductive bias of INRs and show that all modern INRs are intrinsically guided by the same fundamental principles, which let them express a wide range of signals.",
    "84": "However, it also makes them prone to the same type of failure modes.",
    "85": "Our novel framework can be used to design new principled solutions to address these shortcomings, but also simplify the tuning of current INRs.",
    "86": "3. Expressive Power of INRS",
    "87": "We now provide an integrated analysis of the expressive power of INRs.",
    "88": "To that end, we will follow the formulation in Eq.",
    "89": "(1), where, to simplify our derivations, we will restrict ourselves to polynomial activation functions, i.e., nonlinearities of the form $\\rho(x)=\\sum_{k=0}^{K}\\alpha_{k}x^{k}$ Note that this is a very mild assumption, as all analytic activation functions, e.g., sinusoids, can be approximated using polynomials with a naïve Taylor expansion;",
    "90": "and that even the non-differentiable ReLUs can be effectively approximated using Chebyshev polynomials [31].",
    "91": "Note, also, that the sequence of coefficients of the polynomial expansion of most activation functions used in practice decays very rapidly [31].",
    "92": "Now, without loss of generality, let $D=1$ and consider what happens when a single-frequency mapping, i.e. $\\gamma(r)=e^{j\\omega r}$ , goes through such a polynomial activation: The output of the activation consists of a linear combination of the integer harmonics of the input frequency, i.e.,",
    "93": "$\\rho(\\gamma(r))=\\rho(e^{j\\omega r})=\\sum_{k=0}^{K}\\alpha_{k}e^{jk\\omega r}.$",
    "94": "(2)",
    "95": "This harmonic expansion is precisely the mechanism that",
    "96": "controls the frequency representation in INRs. More generally, the mapping $\\gamma(r)$ acts as a collection of single frequency basis, whose spectral support is expanded after each non-linear activation into a collection of higher order harmonics.",
    "97": "This particular structure is shared among all FFNs and SIRENs and it gives rise to the following result regarding their expressive power, i.e. the class of functions that can be represented with these architectures.",
    "98": "Theorem 1. Let $f_{\\theta}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$ be an INR of the form of Eq.",
    "99": "(1) with $\\rho^{(l)}(z)=\\sum_{k=0}^{K}\\alpha_{k}z^{k}$ for $l>1.$ Furthermore, let $\\Omega=[\\Omega_{0},...,\\Omega_{T-1}]^{T}\\in\\mathbb{R}^{T\\times D}$ and $\\phi\\in\\mathbb{R}^{T}$ denote the matrix of frequencies and vector of phases, respectively, used to map the input coordinate $r\\in\\mathbb{R}^{D}$ to $\\gamma(r)=sin(\\Omega r+\\phi).$ This architecture can only represent functions of the form",
    "100": "where",
    "101": "$\\begin{matrix}f_{\\theta}(r)=\\sum_{\\omega' \\in \\mathcal{H}(\\Omega)} c_{\\omega'}sin(\\langle\\omega',r\\rangle+\\phi_{\\omega'}\\end{matrix}),$",
    "102": "",
    "103": "$ \\mathcal{H}(\\Omega)\\subseteq\\{\\sum_{t=0}^{T-1}s_{t}\\Omega_{t}|s_{t}\\in\\mathbb{Z}\\wedge\\sum_{t=0}^{T-1}|s_{t}|\\le K^{L-1}\\}$",
    "104": "Proof. See Appendix.",
    "105": "(3)",
    "106": "(4)",
    "107": "Π",
    "108": "Thm. 1 shows that the expressive power of FFNs and SIRENS is restricted to functions that can be expressed as a linear combination of certain harmonics of the feature mapping $\\gamma(r)$ That is, these architectures have the same expressive power as a structured signal dictionary whose atoms are sinusoids with frequencies equal to sums and differences of the integer harmonics of the mapping frequencies.",
    "109": "Interestingly, an analogous result was also proven for the Multiplicative Filter Networks (MFNs) [16], a proof-of-concept architecture based on a multiplicative connection between layers instead of the usual compositional structure of MLPs.",
    "110": "In particular, it can be shown that MFNs, although very different in structure, are also only able to express linear combinations of certain harmonics of their sinusoidal filters [16], which means that they have the same expressive power as FFNs and SIRENS.",
    "111": "Besides this unification, Thm. 1 also highlights that the way all these architectures encode different signals is very similar.",
    "112": "Indeed, instead of representing a signal by directly learning the coefficients of the linear combination, which would require to store $\\mathcal{O}(TK^{L})$ coefficients $\\{c_{\\omega'}\\};$ the multilayer structure of all INRs imposes a certain low rank structure over the coefficients - akin to the sparsity assumption in classical dictionaries [58] - which can greatly save on memory as it only requires to store $\\mathcal{O}(T^{2}L)$ parameters.",
    "113": "This is better understood through an illustrative example³.",
    "114": "We will refer to these components as the harmonics of $\\gamma(r)$.",
    "115": "Similar examples for other architectures can be found in the Appendix.",
    "116": "3"
  },
  "Page 4": {
    "117": "Example. Let $f_{\\theta}$ be a three-layer SIREN defined as",
    "118": "$f_{\\theta}(r)={w^{(2)}}^{\\top}sin(W^{(1)}sin(\\Omega r))$,",
    "119": "(5)",
    "120": "where $\\Omega\\in\\mathbb{R}^{T}$, $W^{(1)}\in\\mathbb{R}^{F\\times T}$ and $w^{(2)}\in\\mathbb{R}^{F}$ The output of this network can equivalently be represented as",
    "121": "$f_{\\theta}(r)=\\sum_{m=0}^{F-1}\\sum_{s_{1},...,s_{T}=-\\infty}^{\\infty}c_{m,s_{1},...,s_{T}}sin(\\sum_{t=0}^{T-1}s_{t}\\omega_{t}r)$",
    "122": "where",
    "123": "(6)",
    "124": "(7)",
    "125": "and $J_{s}$ denotes the Bessel function of first kind of order s. $c_{m,s_{1},...,s_{T}}=(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)}))w_{m}^{(2)}$",
    "126": "Proof. See Appendix",
    "127": "Π",
    "128": "As we can see, the harmonic expansion introduced by the nested sinusoids of this simple SIREN can be developed into a signal with a very large bandwidth.",
    "129": "Indeed, the few coefficients of this network are enough to represent a signal supported by an infinite number of frequency harmonics.",
    "130": "On the other hand, note that composing sinusoids is a common operation in communication theory as it defines the basis of frequency modulation (FM) technology [46].",
    "131": "Interestingly, drawing analogies between FM signals and SIRENs is a good source of inspiration to intuitively understand how these networks modulate their spectral bias: Recall that for FM signals, such as $sin(\\beta~sin(\\omega_{0}r))$, the parameter $\\beta$ controls the bandwidth of the modulation, which is generally limited by the decreasing nature of the Bessel coefficients $J_{n}(\\beta)$ in n.",
    "132": "Increasing $\\beta$ has the effect of expanding the spectral support of the modulation, as the arguments of the Bessel functions increase.",
    "133": "The analogous phenomenon can be observed in Eq. (6) for this simple SIREN, but can be extended to more general architectures.",
    "134": "In general, we see that due to the decreasing nature of the Bessel functions $J_{s_{t}}(W_{m,t}^{(1)})$, the high order harmonics in Eq.",
    "135": "(6) tend to have smaller weights than the lower ones.",
    "136": "This specific parameterization acts as an implicit bias mechanism, which focuses most of the energy of the output signal in a narrow band around the input frequencies $\\Omega$.",
    "137": "Nevertheless, we can also see that increasing the scale of the coefficients in the inner layer, e.g., $W^{(1)}$, makes the coefficients of higher order terms in Eq.",
    "138": "(7) larger, thus increasing the power of the higher order harmonics, and allowing the network to learn a wider range of frequencies.",
    "139": "The fact that all modern INRs encode information in a similar way can explain why all these architectures are as powerful, in practice.",
    "140": "However, it may also explain why they all suffer from the same failure modes. In Sec.",
    "141": "4, we study these in more detail.",
    "142": "Ground Truth",
    "143": "($f_0 = 1$)",
    "144": "Single frequency mapping ($f_0=0.5$)",
    "145": "FFN ($\sigma = 10$)",
    "146": "FFN",
    "147": "($\\sigma = 10$)",
    "148": "Figure 2. Left Image reconstruction with different mappings of the input coordinates.",
    "149": "Right: Magnitude of the DFT of the reconstruction. The FFN uses random Fourier encodings as defined in Sec.",
    "150": "2, and the single frequency mappings correspond to $\\gamma(r) = [cos(2\\pi f_0 r), sin(2\\pi f_0 r)]$.",
    "151": "4. Failure modes of INRS",
    "152": "We now move on to study of the main failure modes of INRs.",
    "153": "In particular, we will see how the specific harmonic expansion from Thm.",
    "154": "1 can sometimes lead to very recognizable artifacts in the learned reconstructions. Specifically, imperfect signal recovery and aliasing.",
    "155": "4.1. Imperfect recovery",
    "156": "One of the main consequences of Thm. 1 is that the set of frequencies that define the base embedding $\\gamma(r)$ completely determines the frequency support of the reconstruction $f_{\\theta}(r)$.",
    "157": "In this sense, it is fundamental to guarantee that the set $\\mathcal{H}(\\Omega)$ permits to properly cover the spectrum of $g(r)$ When this is not the case, the reconstructed representations can exhibit severe artifacts in the spatial domain stemming from an incorrect choice of fundamental frequencies determined by the INR mapping in Eq.",
    "158": "(1).",
    "159": "Let us illustrate this phenomenon for $FFNs^{4},$ but note that other types of architectures, such as SIRENS, also can suffer from spatial artifacts.",
    "160": "To that end, let us take the extreme case of an FFN $f_{\\theta}:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}^{3},$ with a deterministic single frequency Fourier encoding $\\gamma(r)=$ $[sin(2\\pi f_{0}r),cos(2\\pi f_{0}r)]^{\\top}$ reconstructing an image f: $[-1,1]^{2}\\rightarrow\\mathbb{R}^{3}$ from samples in a grid of $N\\times N$",
    "161": "The details of all our experiments can be found in the Appendix.",
    "162": "We replicate our experiments for other networks in the Appendix.",
    "163": "4"
  },
  "Page 5": {
    "164": "Now, note that, in light of Thm. 1, this network can only represent signals with a frequency support in $\\mathcal{H}(\\Omega)\\subseteq\\{2k\\cdot\\pi f_{0}|k\\in\\mathbb{Z}\\}$, i.e., containing only even multiples of $\\pi f_{0}$ This means that if we choose $f_{0}=1$, the discrete Fourier transform (DFT) of the reconstruction will only have non-zero coefficients at frequencies corresponding to $2k\\cdot2\\pi/N$ for $k=0,...,\\lfloor(N-1)/2\\rfloor$.",
    "165": "This frequency covering is certainly not enough to completely represent images, as it misses all odd multiples of $2\\pi/N$.",
    "166": "As shown in Fig. 2, reconstructing an image with such network produces severe artifacts.",
    "167": "The learned representation with $f_{0}=1$ is highly distorted. That is, we see multiple displaced versions of the target image imposed over each other.",
    "168": "The nature of this artifact is much more clear when we inspect the DFT of the reconstruction, which is supported on a perfect grid in the spectral domain, missing all the values of the spectrum at the odd coefficients.",
    "169": "Strikingly, setting $f_{0}=0.5$ is enough to completely get rid of this type of artifact.",
    "170": "Indeed, when $f=0.5$ the set $\\mathcal{H}(\\Omega)\\subseteq\\{\\pi k|k\\in\\mathbb{Z}\\}$, which means that the DFT of the reconstruction can have energy in all spectral coefficients.",
    "171": "Nonetheless, we also observe that the resulting image is quite blurry.",
    "172": "As we will see, this is due to the fast decay of the polynomial coefficients in Eq.",
    "173": "(2) for most activation functions, including ReLUs [31], which causes the weights of the high frequency harmonics in Eq.",
    "174": "(3) to be very small. This phenomenon can be greatly alleviated, however, by increasing the frequency cover of the initial mapping $\\gamma(r)=sin(\\Omega r+\\phi)$ and sampling $\\Omega\\in\\mathbb{R}^{D\\times T}$ using $\\Omega_{i,j}\\sim\\mathcal{N}(0,\\sigma^{2})$.",
    "175": "Indeed, using a large T with a large $\\sigma$ can reduce the probability of having a limited representation of the frequency spectrum of the target signal.",
    "176": "Nevertheless, as we will see in Sec. 4.2, setting too large $\\sigma$ can introduce other problems.",
    "177": "4.2. Aliasing",
    "178": "It has been empirically shown that INRs with high fundamental frequencies in $\\gamma(r)$ converge faster, and achieve higher performances in the training set [51,56];",
    "179": "even for targets with high frequency details. Nevertheless, it has also been reported that initializing these frequencies too high leads to poor performance outside the exact support of the training set, and produces aliasing artifacts [5].",
    "180": "To the best of our knowledge, this behavior is still poorly understood.",
    "181": "Thm. 1 can, however, shed new light on this phenomenon.",
    "182": "To that end, it is useful to see INRs as digital-to-analog converters (DAC), since INRs do little more than reconstruct a continuous signal from a set of discrete training samples.",
    "183": "Classical sampling theory [40] guarantees that one can reconstruct a bandlimited signal from its samples provided the sampling frequency is above the Nyquist rate.",
    "184": "Nevertheless, it also states that without this prior knowledge, the problem of reconstructing a continuous signal",
    "185": "$\\omega_0= 300$",
    "186": "0m",
    "187": "$\\omega_0= 30$",
    "188": "0m",
    "189": "Magnitude Spectrum (dB)",
    "190": "$f=-23$ $f=23$",
    "191": "0-",
    "192": "-100",
    "193": "-50 0",
    "194": "Magnitude Spectrum (dB) $f=-23$ $f=23$ 50-",
    "195": "0",
    "196": "-50",
    "197": "-f (Hz)",
    "198": "50",
    "199": "Magnitude Spectrum (dB) $f=-23$ 40F-105 20-",
    "200": "-100 0 Magnitude Spectrum (dB) $f=-23$ $f=23$",
    "201": "30- 0 -30-",
    "202": "$f=105$",
    "203": "100",
    "204": "(Hz) $f_{\\theta}(r)$ g(r)",
    "205": "-50 0 50f (Hz) -100 0 100 f (Hz) Sampling frequency $f_{s}=128$ Sampling frequency $f_{s}=256$",
    "206": "Figure 3. Magnitude of the spectrum of $g(r)=sin(2\\pi\\cdot23r)$ and its SIREN reconstruction trained at $f_{s}=128~Hz.$ Top row shows $\\omega_{0}=300$ and bottom row $\\omega_{0}=30.$ On the left the signals are sampled at $f_{s}=128$ Hz and on the right at $f_{s}=256$ Hz.",
    "207": "from its samples is, in general, ill-posed there are many continuous functions that can lead to the exact same samples.",
    "208": "Since INRs do not have an explicit knowledge of the bandwidth of the target, only their implicit bias can determine which of all these functions they reconstruct.",
    "209": "When the implicit bias does not match the nature of the signals, this can lead to reconstruction artifacts.",
    "210": "Take for instance the problem of reconstructing a single-frequency signal $g(r)=sin(2\\pi\\cdot23r)$ using a SIREN $(\\omega_{0}=300$ rad/s) trained on 128 evenly spaced samples in the range [0, 1], i.e., sampled with a frequency of $f_{s}=128$ Hz.",
    "211": "As we can see in Fig. 3, the discrete-time Fourier transform of the reconstruction at the training points perfectly matches the target signal, i.e., the training loss is zero.",
    "212": "Surprisingly, though, if one reconstructs the signal on a finer grid, e.g., $f_{s}=256$ Hz, which contains coordinates not seen during training, one can see that the spectrum of the reconstruction has an additional peak at 105 Hz that is not present in the target signal.",
    "213": "That is, the implicit bias of the network has \"chosen\" to reconstruct the signal using an aliased higher frequency component, as it had no way to discard this feasible solution.",
    "214": "Interestingly, if one initializes the SIREN using $\\omega_{0}=30~rad/s$, instead, this aliased copy disappears.",
    "215": "Thm. 1 gives the key to understand this behaviour. Specifically, note that most non-linearities used in INRs, e.g., ReLU or sin, can be effectively approximated by polynomials of small order, or with rapidly decaying coefficients.",
    "216": "As a result, even if the frequency support of the INRs can include harmonics of very high frequencies, theoretically, those components tend to be weighted with much smaller coefficients in practice.",
    "217": "Increasing the value of the fundamental frequencies does help to include higher frequency components without relying in very high order harmonics.",
    "218": "However, it does so, at the cost of introducing high frequency components with large weights in Eq.",
    "219": "(3), thus increasing the chances of yielding aliased reconstructions.",
    "220": "5"
  },
  "Page 6": {
    "221": "Reconstructing signals at low sampling rates makes the aliased high frequency components in Eq.",
    "222": "(3) indistinguishable from lower frequency components. As we have seen this phenomenon stems from the underspecification [12] of the reconstruction of the reconstruction problem in INRs, which can yield aliasing artifacts when testing at higher sampling rates.",
    "223": "Solving this issues is crucial in application where a certain degree of generalization is required from the INRs.",
    "224": "Applications such as super-resolution [9, 22] or scene reconstruction [51] cannot rely on pure overfitting, and require INRs to generalize outside of their training support.",
    "225": "Overall, we hope that our new insights can support the design of a new generation of INR architectures and algorithms that can mitigate this underspecification.",
    "226": "5. Inductive bias of INRS",
    "227": "All our results, so far, have only dealt with expressive power, i.e., the type of functions that can be represented by INRs.",
    "228": "However, even if a network can express a signal, it does not mean that it can learn to represent it efficiently.",
    "229": "MLPs, for instance, are widely known to be universal function approximators [ ], but still they have a hard time learning to high frequency functions [47].",
    "230": "To the best of our knowledge, the inductive bias of INRs is a largely unexplored topic.",
    "231": "Besides the fact that INRs can circumvent the spectral bias [51,56], little is known of how different design choices influence the learnability of different signals.",
    "232": "In what follows, we will try to narrow this knowledge gap, as we will leverage recent results from deep learning theory to shed new light on the inductive bias of INRs, and how their initialization has a crucial role on what they learn.",
    "233": "5.1. Overview of NTK theory",
    "234": "Studying the inductive bias of deep learning is hard. This is mostly due to the non-linear nature of the mapping between parameters and functions specified by neural networks.",
    "235": "Recent studies, however, have started arguing that studying learnability approximately is much more tractable.",
    "236": "Notably, the neural tangent kernel (NTK) framework [21] proposes to approximate any neural network by its first order Taylor decomposition around the initialization $\\theta_{0}$, i.e.,",
    "237": "$f_{\\theta}(r)\\approx f_{\\theta_{0}}(r)+(\\theta-\\theta_{0})^{\\top}\\nabla_{\\theta}f_{\\theta_{0}}(r)$",
    "238": "(8) since using this approximation, the network is reduced to a simple linear predictor defined by the kernel",
    "239": "$\\Theta(r_1,r_2) = (\\nabla_\\theta f_\\theta (r_1), \\nabla_\\theta f_\\theta (r_2)).$ (9) Remarkably, while the understanding of deep learning is still in its infancy, the learning theory of kernels is much more developed [53].",
    "240": "Specifically, it can be shown that using the kernel in Eq. (9), the sample complexity, and optimization difficulty, of learning a target function g grows proportionally to its kernel norm [6], i.e.,",
    "266": "timization difficulty, of learning a target function g grows proportionally to its kernel norm [6], i.e.,",
    "267": "$||g||_{\\Theta}^{2}=\\sum_{i=0}^{\\infty}\\frac{1}{\\lambda_{i}}|\\langle\\phi_{i},g\\rangle|^{2}$",
    "268": "(10)",
    "269": "where $\\langle\\phi_{i},g\\rangle=\\mathbb{E}_{r}[\\phi_{i}(r)g(r)],$ and $\\{\\lambda_{i},\\phi_{i}\\}_{i=0}^{\\infty}$ denote the eigenvalue, eigenfunction pairs of the kernel given by its Mercer's decomposition, i.e., $\\Theta(r_{1},r_{2})=$ $\\sum_{i=0}^{\\infty}\\lambda_{i}\\phi_{i}(r_{1})\\phi_{i}(r_{2}).$ That is, those targets that are more concentrated in the span of the eigenfunctions associated with the largest eigenvalues of the kernel are easier to learn.",
    "270": "Eq. (8) holds with equality only if the neural network $f_{\\theta}$ is infinitely wide and has a specific structure [1, 21].",
    "271": "For the finite-size neural networks used in practice, it only provides a rough approximation.",
    "272": "Fortunately, recent studies have shown that even if finite-size neural networks and their kernel approximations do not have exactly the same dynamics, their sample complexity when learning a target g scales in both cases with its kernel norm [41], which makes Eq.",
    "273": "(10) a good proxy for learnability in deep learning.",
    "274": "5.2. NTK eigenfunctions as dictionary atoms",
    "275": "The fact that the empirical NTK can approximately capture learnability in deep learning leads to a new interpretation of INRs: we can view INRs as signal dictionaries whose atoms are given by the eigenfunctions of the NTK at initialization.",
    "276": "In this view, the study of the inductive bias of an INR is equivalent to the study of the representation capabilities of its NTK dictionary, in the sense that the functions that can be efficiently encoded by this dictionary are the ones that will be easier to learn.",
    "277": "The simplicity of this analogy allows us to investigate phenomena that appear complex otherwise.",
    "278": "For example, we can use this perspective to constructively characterize the effect of the parameter $\\omega_{0}$ in the inductive bias of a SIREN, and compare different networks, or initializations.",
    "279": "6",
    "241": "100%",
    "242": "Energy concentration $\\mathcal{E}(\\lambda)$",
    "243": "Architecture PSNR 4B",
    "244": "ReLU-MLP",
    "245": "9.11.",
    "246": "SIREN-5",
    "247": "13.56",
    "248": "SIREN-10",
    "249": "15,62",
    "250": "SIREN-20",
    "251": "17.91",
    "252": "SIREN-40",
    "253": "18.56",
    "254": "SIREN-60",
    "255": "16.97",
    "256": "SIREN 80",
    "257": "15.56",
    "258": "SIREN-100",
    "259": "13.96",
    "260": "SIREN-(Meta) 23.06",
    "261": "$10^0$ $10^{-2}$ $10^{-3}$ $10^{-4}$ $10^{-5}$ Normalized eigenvalue $\\lambda=\\lambda_{i}/\\lambda_{0}$",
    "262": "10%",
    "263": "10",
    "264": "Figure 4. Average energy concentration of 100 validation images from CelebA on subspaces spanned by the eigenfunctions of the empirical NTK associated to eigenvalues greater than a given threshold.",
    "265": "Legend shows the average test PSNR after training to reconstruct those images from 50% randomly selected pixels."
  },
  "Page 7": {
    "280": "ReLU",
    "281": "FFN $(\\sigma=1)$",
    "282": "FFN $(\\sigma=10)$",
    "283": "Learned",
    "284": "Initialization",
    "285": "SIREN",
    "286": "$(\\omega_{0}=5)$",
    "287": "SIREN $(\\omega_{0}=30)$",
    "288": "SIREN $(\\omega_{0}=100)$",
    "289": "Figure 5. First eigenfunctions of the empirical NTK of different INRs at initialization.",
    "290": "The first six architectures are initialized as described in Sec.",
    "291": "2. The learned initialization row shows the eigenfunctions of a SIREN initialized after meta-learning on 1,000 training images from the CelebA dataset [28] following the procedure described in [55].",
    "292": "Details of this experiment can be found in the Appendix.",
    "293": "To that end, we measure the average energy concentration of $N=100$ validation images $\\{g_{n}\\}_{n=1}^{N}$ from the CelebA dataset [28] on the span of the eigenfunctions of the NTK associated to eigenvalues greater than a given treshold, i.e.,",
    "294": "$\\mathcal{E}(\\lambda)=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{\\lambda_{i}/\\lambda_{0}\\ge\\lambda}\\frac{|\\langle\\phi_{i},g_{n}\\rangle|^{2}}{|\\langle g_{n},g_{n}\\rangle|^{2}}.$",
    "295": "(11)",
    "296": "This metric is intimately connected to the kernel norm in Eq.",
    "297": "(10), and it can give us a convenient perspective of the region of the NTK spectrum that will represent an image.",
    "298": "The results of this procedure applied to different networks are shown in Fig. 4. Remarkably, for very low values of $\\omega_{0}$, most of the energy of these images is concentrated on the eigenfuctions corresponding to small eigenvalues.",
    "299": "However, as we increase $\\omega_{0}$, the energy concentration gets more skewed towards the eigenfunctions associated with large eigenvalues.",
    "300": "Interestingly, after some point $(\\omega_{0}>40)$, the energy profile starts receding to the right, again.",
    "301": "Comparing the energy profiles with the generalization performance of these networks, we observe a clear pattern: the more energy is concentrated on the eigenfunctions associated with larger eigenvalues, the better the test peak signal-to-noise ratio $(PSNR)^{7}$.",
    "302": "To understand this phenomenon, we can inspect the eigenfunctions of the NTK.",
    "303": "As it is shown in Fig. 5, the eigenfunctions of the SIRENS with larger $\\omega_{0}$ have higher frequency content.",
    "304": "This means that increasing $\\omega_{0}$ can have a positive effect in generalization as it yields a dictionary that better spans the medium-high frequency spectrum of natural images.",
    "305": "Increasing $\\omega_{0}$ too much, on the other hand, yields atoms with an overly high frequency content that cannot span the space of natural images efficiently, which explains their poor reconstruction",
    "306": "Details of the experiments can be found in the Appendix. Correlations with other training metrics are shown in the Appendix.",
    "307": "7",
    "308": "performance of these networks.",
    "309": "Overall, we see how interpreting learnability as encoding efficiency of the NTK dictionary is a powerful analogy that can explain diverse phenomena, and lets us study under a single framework all sorts of INR questions, including those which might not be readily understood from Thm.",
    "310": "1. This is a very powerful tool that we further exploit in Sec.",
    "311": "5.3 to provide novel insights on the role of meta-learning in INRs.",
    "312": "5.3. Meta-learning as dictionary learning",
    "313": "Prior work has shown that a correct initialization is key to ensure a good performance for INRs [51,56].",
    "314": "In this sense, recent studies [50, 55] have shown that the use of learned initialization, such as the ones obtained from meta-learning algorithms [17], can significantly boost the performance of INRs.",
    "315": "Indeed, initializing with meta-learned weights is one of the most effective remedies against the slow speed of convergence, and high sample complexity of INRs.",
    "316": "However, while there has been recently great progress in understanding traditional forms of deep learning, the role of meta-learning on the inductive bias of deep neural networks remains largely overlooked.",
    "317": "Interestingly, we now show how using the connections between INRs and signal dictionaries can help us understand meta-learning in general.",
    "318": "To do so, we follow the same experimental protocol as in Sec.",
    "319": "5.2, where instead of computing the eigenfunctions of the NTK at a random initialization point, we linearize the INRs using Eq.",
    "320": "(8) at the meta-learned weights, after pre-training on 1,000 training images from CelebA using model agnostic meta-learning (MAML) [17,55].",
    "321": "As it is shown in Fig. 4, the meta-learned weights yield an eigenstructure that concentrates most of the energy of the target images on a subspace spanned by the eigenfunctions of the NTK with the largest eigenvalues, with almost no energy concentrated on the eigenfunctions corresponding to"
  },
  "Page 8": {
    "322": "smaller eigenvalues. Therefore, training this INR starting from the meta-learned weights, results in a very fast speed of convergence and superior generalization capacity.",
    "323": "As it happened with the role of $\\omega_{0}$ in Sec.",
    "324": "5.2, visually inspecting the eigenfunctions of the NTK can help to build an intuition around this phenomenon.",
    "325": "In this regard, recall that the CelebA dataset consists of a collection of face images.",
    "326": "Strikingly, as illustrated in Fig. 5, the first eigenfunctions of the meta-learned NTK also look like faces.",
    "327": "Clearly, meta-learning has reshaped the NTK so that the eigenfunctions have a large correlation with the target images.",
    "328": "To the best of our knowledge, we are the first to report the NTK reshaping behavior of meta-learning, which cannot be obviously explained by first order approximation theories (cf. Eq. (8)).",
    "329": "This result is remarkable for deep learning theory, as it helps us undertand the high-order dynamics of the NTK during training, which remains one of the main open questions of the field.",
    "330": "Prior work had observed that standard training procedures change the first few eigenfunctions of the NTK so that they look like the target task [4, 25, 41, 42], but our observations in Fig. 4 and Fig. 5 go one step further, and show that meta-learning has the potential to reshape a much larger space of the NTK dictionary by combining many tasks together, thus increasing the capacity of the NTK to efficiently encode a full meta-distribution of signals.",
    "331": "In this sense, we believe that that drawing parallels between classical dictionary learning algorithms [58] and meta-learning can be a strong abstraction which can simplify the complexity of this problem, thus leading to a promising avenue for future research.",
    "332": "Delving deeper in this connection will not only improve our understanding of meta-learning as a whole, but it can also provide new insights for the design of more efficient INRs by leveraging data to construct richer dictionaries.",
    "333": "6. Related work",
    "334": "INRs are a very active research field in computer vision, as they have become integral parts of many applications such as volume reconstruction [33,43], scene rendering [34,37,52], texture synthesis [20, 39], generative modelling [8, 10,36], or compression [14].",
    "335": "Recent architectural advances have focused mostly on improving the inference and training cost [13, 27, 35, 45, 50, 55] of INRs, as well as on mitigating aliasing and improving generalization [5,32].",
    "336": "The theory behind INRs has attracted much less attention, however. Similar to our work, Fathony et al.",
    "337": "studied the expressive power of INRs, but their results only apply to their proposed multiplicative filter network architecture [16].",
    "338": "Zheng et al. [59], on the other hand, studied the trade-off between the rank and distance-preserving properties of different activation functions on INRs.",
    "339": "Most",
    "340": "In the Appendix we provide a more detailed experimental discussion.",
    "341": "notably, however, Tancik et al. [56] showed that precoding the input of an infinitely wide ReLU-network with random Fourier features [48] is equivalent to using a tunable shift-invariant kernel method.",
    "342": "This gives a static intuition of how randomly initialized FFNs circumvent the spectral bias [47].",
    "343": "Our work goes one step further, and builds upon recent empirical results [41] to extend this NTK analysis to finite networks with arbitrary weights and activations, e.g., meta-learned SIRENs.",
    "344": "This allows us to investigate dynamical aspects of INRs such as the role of pre-training.",
    "345": "Interestingly, Kopitkov and Indelman [25] also used the visualization of the eigenfunctions of the NTK during training to understand other high-order training effects, such as the increase of alignment of the NTK with the target signal [4, 25, 41, 42].",
    "346": "Our experiments use a similar approach to show the complex dictionary learning behaviour of MAML [17] in the NTK, which to the best of our knowledge is the first time this has been reported in the literature.",
    "347": "Connected to Thm. 1, other works have also used a similar harmonic expansion to analyze certain effects in deep learning, such as the increase in roughness of the loss landscape with respect to the weights for deeper layers [31], or how skip-connections can avoid shattered gradients [3].",
    "348": "Finally, we note that most of our work draws inspirations from the classical signal processing literature [40].",
    "349": "Some of our derivations are intimately connected to standard techniques in communications [46], and most of our analogies are founded on the field of signal representation [29] and dictionary design [58].",
    "350": "Moving forward, delving deeper on these connections will be a fruitful avenue for future work.",
    "351": "7. Conclusion",
    "352": "In this paper, we have analyzed the expressive power and inductive bias of modern INRs from a unified perspective.",
    "353": "We have shown that the expressive power of a large class of INRs with sinusoidal encodings is given by the space of linear combinations of the integer harmonics of their input mapping.",
    "354": "This allows INRs to encode signals with an exponentially large frequency support using a few coefficients, but also cause them to suffer from imperfect signal recovery or aliasing.",
    "355": "We have also seen that the inductive bias of INRs is captured by the ability of the empirical NTK to encode signals efficiently, and we have revealed that meta-learning can modify the NTK and increase this efficieny.",
    "356": "A natural future extension would be to generalize Thm. 1 to input mappings beyond sinusoids [5, 16] or include normalization layers [2].",
    "357": "Similarly, one could also study the effect of out-of-distribution data on the alignment with the NTK after meta-training.",
    "358": "Finally, it is important to note that our insights should be readily extensible to higher dimensional settings, although most of our practical results were performed using one or two-dimensional signals.",
    "359": "In this sense, designing methods",
    "360": "8"
  },
  "Page 9": {
    "361": "to visualize the eigenfunctions of the NTK in higher dimensions would clearly help to inform practitioners about the inductive bias of different INRs.",
    "362": "Acknowledgements",
    "363": "We thank Alessandro Favero, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli and Arun Venkitaraman for their fruitful discussions and feedback.",
    "364": "This work has been partially supported by a GCP Research Credit Award.",
    "365": "References",
    "366": "[1] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.",
    "367": "On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 6",
    "368": "[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. stat, 1050:21, 2016. 8",
    "369": "[3] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.",
    "370": "The shattered gradients problem: If resnets are the answer, then what is the question?",
    "371": "In International Conference on Machine Learning (ICML), 2017. 8",
    "372": "[4] Aristide Baratin, Thomas George, César Laurent, R. Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and Simon Lacoste-Julien.",
    "373": "Implicit regularization via neural feature alignment. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2021. 8",
    "374": "[5] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.",
    "375": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.",
    "376": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5, 8",
    "377": "[6] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results.",
    "378": "In Computational Learning Theory (COLT), 2001. 6",
    "379": "[7] James Bradbury, Roy Frostig. Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.",
    "380": "JAX: composable transformations of Python+NumPy programs, 2018.",
    "381": "[8] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.",
    "382": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.",
    "383": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 8",
    "384": "[9] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function.",
    "385": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6",
    "386": "[10] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling.",
    "387": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1,8",
    "388": "[11] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303-314, 1989.6",
    "389": "[12] Alexander D'Amour, Katherine A. Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yi-An Ma, Cory Y. McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley.",
    "390": "Underspecification presents challenges for credibility in modern machine learning. arXiv preprint arXiv:2011.03395, 2020. 6",
    "391": "[13] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.",
    "392": "Depth-supervised nerf: Fewer views and faster training for free. arXiv preprint arXiv: 2107.02791, 2021. 8",
    "393": "[14] Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. COIN: COmpression with implicit neural representations.",
    "394": "In Neural Compression: From Information Theory to Applications ICLRW, 2021. 1,8",
    "395": "[15] Emilien Dupont, Yee Whye Teh, and Arnaud Doucet. Generative models as distributions of functions.",
    "396": "arXiv preprint arXiv:2102.04776, 2021. 1",
    "397": "[16] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks.",
    "398": "In International Conference on Learning Representations (ICLR), 2021. 3, 8",
    "399": "[17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks.",
    "400": "In International Conference on Machine Learning (ICML). 2017. 7,8",
    "401": "[18] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin.",
    "402": "Fastnerf: High-fidelity neural rendering at 200fps. arXiv:2103.10380, 2021. 2",
    "403": "[19] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec.",
    "404": "Baking neural radiance fields for real-time view synthesis.",
    "405": "arXiv preprint",
    "406": "arXiv:2103.14645, 2021. 2",
    "407": "[20] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Learning a neural 3d texture space from 2d exemplars.",
    "408": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 8",
    "409": "[21] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks.",
    "410": "In Advances in Neural Information Processing Systems, 2018. 2, 6",
    "411": "[22] Jaechang Kim, Yunjoo Lee. Seunghoon Hong, and Jungseul Ok.",
    "412": "Learning continuous representation of audio for arbitrary scale super resolution. arXiv preprint arXiv:2111.00195, 2021. 6",
    "413": "[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.",
    "414": "arXiv preprint arXiv: 1412.6980, 2014. 19, 21, 22",
    "415": "9"
  },
  "Page 10": {
    "416": "[24] Sylwester Klocek, Łukasz Maziarka, Maciej Wołczyk, Jacek Tabor, Jakub Nowak, and Marek Śmieja. Hypernetwork functional image representation.",
    "417": "In International Conference on Artificial Neural Networks (ICANN), 2019. 1",
    "418": "[25] Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study.",
    "419": "In International Conference on Artificial Neural Networks (ICANN), 2020. 8",
    "420": "[26] Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study.",
    "421": "In International Conference on Artificial Neural Networks (ICANN), 2020. 24",
    "422": "[27] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields.",
    "423": "In Advances in Neural Information Processing Systems (NeurIPS), 2020. 8",
    "424": "[28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.",
    "425": "In Proceedings of International Conference on Computer Vision (ICCV), 2015. 7,26",
    "426": "[29] Stéphane Mallat. A wavelet tour of signal processing. Elsevier, 1999. 8",
    "427": "[30] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth.",
    "428": "Nerf in the wild: Neural radiance fields for unconstrained photo collections.",
    "429": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1",
    "430": "[31] Christian H.X. Ali Mehmeti-Göpel, David Hartmann, and Michael Wand. Ringing relus: Harmonic distortion analysis of nonlinear feedforward networks.",
    "431": "In International Conference on Learning Representations (ICLR), 2021. 3, 5, 8",
    "432": "[32] Ishit Mehta, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker.",
    "433": "Modulated periodic activations for generalizable local functional representations. arXiv preprint arXiv: 2104.03960, 2021. 8",
    "434": "[33] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.",
    "435": "Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1,8",
    "436": "[34] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.",
    "437": "Nerf: Representing scenes as neural radiance fields for view synthesis.",
    "438": "In Proceedings of European Conference on Computer Vision (ECCV), 2020. 1, 3, 8",
    "439": "[35] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger.",
    "440": "DONERF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum, 2021.8",
    "441": "[36] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields.",
    "442": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 8",
    "443": "[37] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger.",
    "444": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In",
    "445": "10",
    "446": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 8",
    "447": "[38] Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, and Samuel S. Schoenholz.",
    "448": "Neural tangents: Fast and easy infinite neural networks in python. In International Conference on Learning Representations (ICLR), 2020. 22",
    "449": "[39] Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas Geiger.",
    "450": "Texture fields: Learning texture representations in function space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.8",
    "451": "[40] Alan V Oppenheim. Discrete-time signal processing. Pearson Education India, 1999. 5,8",
    "452": "[41] Guillermo Ortiz-Jiménez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can linearized neural networks actually say about generalization?",
    "453": "In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2, 6,8.25",
    "454": "[42] Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart.",
    "455": "Geometric compression of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2021. 8",
    "456": "[43] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.",
    "457": "Deepsdf: Learning continuous signed distance functions for shape representation.",
    "458": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 8",
    "459": "[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla.",
    "460": "Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 1",
    "461": "[45] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz.",
    "462": "Hypernerf: A higher-dimensional representation for topologically varying neural",
    "463": "radiance fields. arXiv preprint arXiv: 2106.13228, 2021. 8",
    "464": "[46] John G. Proakis and Masoud Salehi. Fundamentals of Communication Systems. Pearson Education Limited, 2014. 4, 8",
    "465": "[47] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville.",
    "466": "On the spectral bias of neural networks. In International Conference on Machine Learning (ICML), 2019. 1, 2, 6, 8",
    "467": "[48] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.",
    "468": "In Advances in Neural Information Processing Systems (NeurIPS), 2008. 2, 8",
    "469": "[49] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger.",
    "470": "Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps, 2021. 2",
    "471": "[50] Vincent Sitzmann, Eric R Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf: Meta-learning signed distance functions.",
    "472": "arXiv preprint arXiv: 2006.09662, 2020. 2, 7, 8"
  },
  "Page 11": {
    "473": "[51] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions.",
    "474": "In Advances in Neural Information Processing Systems (NeurIPS), 2020. 1, 2, 3, 5, 6, 7, 19, 20, 22, 26",
    "475": "[52] Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations.",
    "476": "arXiv preprint arXiv: 1906.01618, 2019. 1, 8",
    "477": "[53] Alexander J Smola and Bernhard Schölkopf. On a kernel-based method for pattern recognition, regression, approximation, and operator inversion.",
    "478": "Algorithmica, 22(1):211-231.1998.6",
    "479": "[54] Pratul P Srinivasan, Boyang Deng. Xiuming Zhang. Matthew Tancik, Ben Mildenhall, and Jonathan T Barron.",
    "480": "Nerv: Neural reflectance and visibility fields for relighting and view synthesis.",
    "481": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.1",
    "482": "[55] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren Ng.",
    "483": "Learned initializations for optimizing coordinate-based neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 7, 8, 22, 26",
    "484": "[56] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.",
    "485": "Fourier features let networks learn high frequency functions in low dimensional domains.",
    "486": "In Advances in Neural Information Processing Systems (NeurIPS), 2020. 1, 2, 5, 6, 7, 8, 20, 21",
    "487": "[57] A. Tewari, O. Fried, J. Thies, V. Sitzmann, S. Lombardi, Z. Xu, T. Simon, M. Nießner, E. Tretschk, L. Liu, B. Mildenhall, P. Srinivasan, R. Pandey, S. Orts-Escolano, S. Fanello, M. Guo, G. Wetzstein, J.-Y.",
    "488": "Zhu, C. Theobalt, M. Agrawala, D. B Goldman, and M. Zollhöfer. Advances in neural rendering.",
    "489": "In ACM SIGGRAPH 2021 Courses, SIGGRAPH 21, 2021. 1",
    "490": "[58] Ivana Tošić and Pascal Frossard. Dictionary learning. IEEE Signal Processing Magazine, 28(2):27-38, 2011. 3, 8",
    "491": "[59] Jianqiao Zheng, Sameera Ramasinghe, and Simon Lucey. Rethinking positional encoding. arXiv preprint arXiv:2107.02561, 2021. 8",
    "492": "11"
  },
  "Page 12": {
    "493": "Appendix",
    "494": "The following table: \"A Deferred proofs\", \"13\" \"A.1. Proof of Theorem 1.\", \"13\" \"A.2 Two-layer SIREN example\", \"17\" \"B Imperfect recovery\", \"19\" \"B. 1. Experimental details\", \"19\" \"B.2 Additional experiments\", \"19\" \"C Aliasing\", \"21\" \"C.1. Experimental details\", \"21\" \"C.2 Additional experiments\", \"21\" \"D NTK eigenfunctions as dictionary atoms\", \"22\" \"D.1. Estimation of eigenfunctions of the NTK\", \"22\" \"D.2 Training details\", \"22\" \"D.3 Experiments on additional networks\", \"22\" \"E. Meta-learning experiment E.1. Experimental details\", \"22\" \"E.2. Experiments with an additional meta-learning algorithm E.3. Experiments on additional networks\", \"23\" \"E.4. Comparison with standard training\", \"24\"",
    "495": "F. Performance on NTK eigenfunctions",
    "496": "25",
    "497": "12"
  },
  "Page 13": {
    "498": "A. Deferred proofs",
    "499": "A.1. Proof of Theorem 1",
    "500": "We provide here the proof of Theorem 1 which gives an explicit expression to the expressive power of INRs.",
    "501": "However, before we delve deeper in this proof we will prove a few useful lemmas.",
    "502": "A.1.1 Preliminary lemmas",
    "503": "Lemma 1. Let $\\{\\omega_{k}^{(1)}\\in\\mathbb{R}^{D}\\}_{k\\in K}$ and $\\{\\omega_{j}^{(2)}\\in\\mathbb{R}^{D}\\}_{j\\in\\mathcal{J}}$, and $\\{\\phi_{k}^{(1)}\\in\\mathbb{R}\\}_{k\\in\\mathcal{K}}$ and $\\{\\phi_{j}^{(2)}\\in\\mathbb{R}\\}_{j\\in J}$ be two collections of frequency vectors and scalar phases, respectively, indexed by the sets $\\mathcal{K},\\mathcal{J}\\subseteq\\mathbb{N}.$ Furthermore, let $\\{\\beta_{k}^{(1)}\\in\\mathbb{R}\\}_{k\\in K}$ and $\\{\\beta_{j}^{(2)}\\in\\mathbb{R}\\}_{j\\in J}$ be two sets of scalar coefficients and $r\\in\\mathbb{R}^{D}$ Then,",
    "504": "where",
    "505": "$(\\sum_{k\\in K}\\beta_{k}^{(1)}cos(\\langle\\omega_{k}^{(1)},r\\rangle+\\phi_{k}^{(1)}))(\\sum_{j\\in\\mathcal{J}}\\beta_{j}^{(2)}cos(\\langle\\omega_{j}^{(2)},r\\rangle+\\phi_{j}^{(2)}))=\\sum_{\\omega'\\in\\mathcal{D}}\\tilde{\\beta}_{\\omega'}cos(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega'}).$",
    "506": "$\\mathcal{D}(\\{\\omega_{k}^{(1)}\\}_{k\\in K},\\{\\omega_{j}^{(2)}\\}_{j\\in\\mathcal{J}})=\\{\\omega'=\\omega_{k}^{(1)}\\pm\\omega_{j}^{(2)}|k\\in K,j\\in\\mathcal{J}\\}$",
    "507": "for some $\\{\\overline{\\phi}\\omega'\\in\\mathbb{R}|\\omega'\\in\\mathcal{D}\\}.\\{\\overline{\\beta}\\omega'\\}\\in\\mathbb{R}|\\omega'\\in\\mathcal{D}\\}$",
    "508": "Proof.",
    "509": "$(\\sum_{k\\in\\mathcal{K}}\\beta_{k}^{(1)}cos(\\langle\\omega_{k}^{(1)},r\\rangle+\\phi_{k}^{(1)}))(\\sum_{j\\in\\mathcal{J}}\\beta_{j}^{(2)}cos(\\langle\\omega_{j}^{(2)},r\\rangle+\\phi_{j}^{(2)}))$",
    "510": "$=\\sum_{k\\in K}\\sum_{j\\in\\mathcal{J}}\\beta_{k}^{(1)}\\beta_{j}^{(2)}cos(\\langle\\omega_{k}^{(1)},r\\rangle+\\phi_{k}^{(1)})cos(\\langle\\omega_{j}^{(2)},r\\rangle+\\phi_{j}^{(2)})$",
    "511": "$=\\sum_{k\\in K}\\sum_{j\\in J}\\beta_{k}^{(1)}\\beta_{j}^{(2)}\\frac{1}{2}(cos(\\langle\\omega_{k}^{(1)},r\\rangle+\\langle\\omega_{j}^{(2)},r\\rangle+\\phi_{k}^{(1)}+\\phi_{j}^{(2)})+cos(\\langle\\omega_{k}^{(1)},r\\rangle-\\langle\\omega_{j}^{(2)},r\\rangle+\\phi_{k}^{(1)}-\\phi_{j}^{(2)})) = \\sum_{k\\in K}\\sum_{j\\in\\mathcal{J}}\\beta_{k}^{(1)}\\beta_{j}^{(2)}\\frac{1}{2}(cos(\\langle\\omega_{k}^{(1)}+\\omega_{j}^{(2)},r\\rangle+\\phi_{k}^{(1)}+\\phi_{j}^{(2)})+cos(\\langle\\omega_{k}^{(1)}-\\omega_{j}^{(2)},r\\rangle+\\phi_{k}^{(1)}-\\phi_{j}^{(2)}))$",
    "512": "(12)",
    "513": "(13)",
    "514": "$=\\sum_{\\omega'\\in\\mathcal{D}}\\tilde{\\beta}\\omega'cos(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega'})$",
    "515": "(14)",
    "516": "Π",
    "517": "Lemma 2. Let $\\{\\omega_{j}\\in\\mathbb{R}^{D}\\}_{j\\in J}$ and $\\{\\phi_{j}\\in\\mathbb{R}\\}_{j\\in\\mathcal{J}}$ be a collection of frequency vectors and scalar phases, respectively, indexed by the set $\\mathcal{J}\\subseteq\\mathbb{N}$ Furthermore, $\\{\\beta_{j}\\in\\mathbb{R}\\}_{j\\in J}$ be a set of scalar coefficients, and let $k\\in\\mathbb{N}$ Then,",
    "518": "where",
    "519": "$(\\sum_{j\\in\\mathcal{J}}\\beta_{j}cos(\\langle\\omega_{j},r\\rangle+\\phi_{j}))^{k}=\\sum_{\\omega'\\in\\mathcal{H}_{k}}\\tilde{\\beta}\\omega'cos(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega'})$",
    "520": "$\\mathcal{H}_{k}(\\{\\omega_{j}\\}_{j\\in\\mathcal{J}})\\subseteq\\tilde{\\mathcal{H}}_{k}(\\{\\omega_{j}\\}_{j\\in\\mathcal{J}}):=\\{\\omega'=\\sum_{j\\in\\mathcal{J}}c_{j}\\omega_{j}|c_{j}\\in\\mathbb{Z}\\wedge\\sum_{j\\in\\mathcal{J}}|c_{j}|\\le k\\}$",
    "521": "(15)",
    "522": "(16)",
    "523": "Note that we will often use the notation $\\mathcal{H}_{k}$ and $\\tilde{\\mathcal{H}}_{k}$ instead of explicitly writing the dependence on the set $(\\{\\omega_{j}\\}_{j\\in J})$ when it is clear from the context.",
    "524": "Proof. The statement trivially holds for $k=1.$ Assume it also holds for k, then",
    "525": "13"
  },
  "Page 14": {
    "526": "$(\\sum_{j\\in J}\\beta_{j}cos(\\langle\\omega_{j},r\\rangle+\\phi_{j}))^{k+1}=(\\sum_{j\\in J}\\beta_{j}cos(\\langle\\omega_{j},r\\rangle+\\phi_{j}))^{k}(\\sum_{j\\in J}\\beta_{j}cos(\\langle\\omega_{j},r\\rangle+\\phi_{j}))$",
    "527": "(17)",
    "528": "(19)",
    "529": "$=(\\sum_{\\omega'\\in\\mathcal{H}_{k}}\\tilde{\\beta}_{\\omega'}cos(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega'}))(\\sum_{j\\in\\mathcal{J}}\\beta_{j}cos(\\langle\\omega_{j},r\\rangle+\\dot{\\phi}_{j}))$ (18) $=\\sum_{\\omega'\\in\\mathcal{D}\\{\\mathcal{H}_{k},\\{\\omega_{j}\\}_{j\\in\\mathcal{J}}\\}}\\beta_{\\omega'}'cos(\\langle\\omega',r\\rangle+\\phi_{\\omega'}')$ $=\\sum_{\\omega'\\in\\mathcal{H}_{k+1}}\\beta_{\\omega'}'cos(\\langle\\omega',r\\rangle+\\phi_{\\omega'}')$",
    "530": "where Eq. (18) holds by assumption and Eq. (19) holds because of the previous lemma. Moreover we have",
    "531": "$\\mathcal{H}_{k+1}=D\\{\\mathcal{H}_{k},\\{\\omega_{i}\\}_{i\\in J}\\}=\\{\\omega'=\\omega_{h}\\pm\\omega_{i}|\\omega_{h}\\in\\mathcal{H}_{k},i\\in\\mathcal{J}\\}$",
    "532": "$\\subseteq\\{\\omega'=\\sum_{j\\in\\mathcal{J}}c_{j}\\omega_{j}\\pm\\omega_{i}|c_{j}\\in\\mathbb{Z}\\wedge\\sum_{j\\in\\mathcal{J}}|c_{j}|\\le k,i\\in\\mathcal{J}\\} \\subseteq\\{\\omega'=\\sum_{j\\in\\mathcal{J}}c_{j}\\omega_{j}|c_{j}\\in\\mathbb{Z}\\wedge\\sum_{j\\in\\mathcal{J}}|c_{j}|\\le k+1\\}$",
    "533": "So Eq. (15) holds for $k+1$ as well. Then by induction Eq. (15) holds $\\forall k\\in\\mathbb{N}$",
    "534": "A.1.2 Main proof",
    "535": "(20)",
    "536": "(21)",
    "537": "(22)",
    "538": "(23)",
    "539": "(24)",
    "540": "(25)",
    "541": "Π",
    "542": "Recall that we are interested in understanding the expressive power of INR architectures that can be decomposed into a mapping function $\\gamma:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{T}$ followed by a multilayer perceptron (MLP), with weights $W^{(l)}\\in\\mathbb{R}^{F_{l-1}\\times F_{l}}$ bias $b^{(l)}\\in\\mathbb{R}^{F_{l}},$ and activation function $\\rho^{(l)}:\\mathbb{R}\\rightarrow\\mathbb{R}$ applied elementwise;",
    "543": "at each layer $l=1,...,L-1$ That is, if we denote by $z^{(l)}$ each layers post activation, most INR architectures compute",
    "544": "$z^{(0)}=\\gamma(r)$,",
    "545": "$z^{(l)}=\\rho^{(l)}(W^{(l)}z^{(l-1)}+b^{(l)})$, $l=1,...,L-1$",
    "546": "(26)",
    "547": "$f_{\\theta}(r)=W^{(L)}z^{(L-1)}+b^{(L)}$.",
    "548": "Based on this architecture we can prove the following theorem.",
    "549": "$\\rho^{(l)}(z)=\\sum_{k=0}^{K}\\alpha_{k}z^{k}$",
    "550": "Theorem. Let $f_{\\theta}:\\mathbb{R}^{D}\\rightarrow$ be an INR of the form of Eq.",
    "551": "(26) with for $l>1$ Furthermore, let $\\Omega=$ $[\\Omega_{0},...,\\Omega_{T-1}]^{T}\\in\\mathbb{R}^{T\\times D}$ and $\\phi\\in\\mathbb{R}^{T}$ denote the matrix of frequencies and vector of phases, respectively, used to map the input coordinate $r\\in\\mathbb{R}^{D}to~\\gamma(r)=sin(\\Omega r+\\phi)$.",
    "552": "This architecture can only represent functions of the form",
    "553": "where",
    "554": "$\\mathcal{H}(\\Omega)$",
    "555": "$f_{\\theta}(r)=\\sum_{\\omega'\\in\\mathcal{H}(\\Omega)}c_{\\omega'}sin(\\langle\\omega',r\\rangle+\\phi_{\\omega'})$ ,",
    "556": "$T-1$",
    "557": "$T-1$",
    "558": "$\\omega' = \\sum s_t \\Omega_t | s_t \\in \\mathbb{Z} \\wedge \\sum |s_t| \\le K^{L-1}$",
    "559": "$t=0$",
    "560": "$t=0$",
    "561": "(27)",
    "562": "(28)",
    "563": "Proof. We will prove the statement by induction. To that end, let us denote the preactivation vector at each layer as $v^{(l)}$, i.e. $z^{(l)}=$ $\\rho^{(l)}(v^{(l)})$.",
    "564": "We will first derive the expressions for the base case.",
    "565": "14"
  },
  "Page 15": {
    "566": "Base case Consider the preactivation of a node at the first layer of the neural network for any mapping of the form in Eq.",
    "567": "(26). Then",
    "568": "$v_{j}^{(1)}=W_{j}^{(1)}\\gamma(r)=\\sum_{t=0}^{T-1}b_{tj}cos(\\langle\\Omega_{t},r\\rangle+\\phi_{tj})$",
    "569": "(29)",
    "570": "with some $b_{tj}\\in\\mathbb{R}$ and $\\phi_{t_{j}}\\in\\mathbb{R}$ depending on the first layer weights connected to that node.",
    "571": "Also note that interchanging sines with cosines only affects the phase terms.",
    "572": "Therefore, using the result of Lemma 2, and after applying the activation function, the output of each node at the first layer is given by",
    "573": "$z_{j}^{(1)}=\\rho^{(1)}(v_{j}^{(1)})=\\sum_{k=0}^{K}\\alpha_{k}(v_{j}^{(1)})^{k}=\\sum_{k=0}^{K}\\alpha_{k}(\\sum_{t=0}^{T-1}b_{tj}cos(\\langle\\Omega_{t},r\\rangle+\\phi_{tj}))^{k}$",
    "574": "$=\\sum_{k=0}^{K}\\alpha_{k}\\sum_{\\omega_{k}'\\in\\mathcal{H}_{k}}\\beta\\omega_{k}'}cos(\\langle\\omega_{k}',r\\rangle+\\phi\\omega_{k}')$ $=\\sum_{\\omega_{k}\\in\\mathcal{H}_{k}'}\\overline{\\beta}_{\\omega_{k}}cos(\\langle\\omega_{k},r\\rangle+\\overline{\\phi}_{\\omega_{k}})$",
    "575": "(30)",
    "576": "(31)",
    "577": "(32)",
    "578": "and we use the definitions of $\\mathcal{H}_{k}$ and $\\tilde{\\mathcal{H}}_{k}$ in Lemma 2. Therefore, since $\\forall k\\mathcal{H}_{k}\\subseteq\\tilde{\\mathcal{H}}_{k}$ by construction, and $\\forall j\\le k$",
    "579": "where",
    "580": "$\\mathcal{H}_{K}':=\\bigcup_{j=1}^{K}\\mathcal{H}_{j}$",
    "581": "$\\mathcal{H}_{j}\\subseteq\\overline{\\mathcal{H}_{k}}$ then it holds that $\\mathcal{H}_{K}'\\subseteq\\tilde{\\mathcal{H}}_{K}$ i.e.,",
    "582": "$\\mathcal{H}_{K}'\\subseteq\\tilde{\\mathcal{H}}_{K}=\\{\\omega'=\\sum_{t=0}^{T-1}s_{t}\\Omega_{t}|s_{t}\\in\\mathbb{Z}\\wedge\\sum_{t=0}^{T-1}|s_{t}|\\le K\\}$",
    "583": "Induction step Assume the output of the nodes at layer l satisfy the following expression:",
    "584": "where",
    "585": "$z_{j}^{(l)}=\\sum_{\\omega'\\in\\mathcal{H}^{(l)}(\\Omega)}c_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\phi_{\\omega',j})$",
    "586": "$\\mathcal{H}^{(l)}\\subseteq\\tilde{\\mathcal{H}}_{K^{l}}=\\{\\omega'=\\sum_{t=0}^{T-1}s_{t}\\Omega_{t}|s_{t}\\in\\mathbb{Z}\\wedge\\sum_{t=0}^{T-1}|s_{t}|\\le K^{l}\\}$",
    "587": "Then, the preactivation of any node at the",
    "588": "$(l+1)^{tn}$",
    "589": "layer can be expressed as:",
    "590": "$v_{j}^{(l+1)}=\\sum_{\\omega'\\in\\mathcal{H}^{(l)}(\\Omega)}b\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j})$",
    "591": "(33)",
    "592": "(34)",
    "593": "(35)",
    "594": "(36)",
    "595": "since the sum of cosines with the same frequency only result in a cosine with the same frequency but with a modified phase and amplitude.",
    "596": "Hence, after applying the activation function the output of the $j^{th}$ node at the $(l+1)^{th}$ layer can be written as:",
    "597": "$z_{j}^{(l+1)}=\\rho^{(l+1)}(v_{j}^{(l+1)})=\\sum_{k=0}^{K}\\alpha_{k}(v_{j}^{(l+1)})^{k}=\\sum_{k=0}^{K}\\alpha_{k}(\\sum_{\\omega'\\in\\mathcal{H^{(l)}(\\Omega)}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j}))^{k}$",
    "598": "Let us inspect the term",
    "599": "$(\\sum\\omega'\\in\\mathcal{H}^{(l)}(\\Omega)}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\overline{\\phi}_{\\omega',j}))^{k}$",
    "600": "(37)",
    "601": ". Instead of directly applying Lemma 2, we will leverage the fact that",
    "602": "all the frequencies $\\omega'\\in\\mathcal{H}^{(l)}$ share a similar structure. More precisely, they all can be represented as a sum of the frequencies in the set $\\Omega$.",
    "603": "To that end, let us show the following intermediate result:",
    "604": "$(\\sum_{\\omega'\\in\\mathcal{H}^{(l)}(\\Omega)}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j}))^{k}=\\sum_{\\omega'\\in\\mathcal{H}_{k}^{(l)}\\tilde{b}\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\tilde{\\phi}}\\omega',j})$ 15"
  },
  "Page 16": {
    "605": "(38)",
    "606": "where $\\mathcal{H}_{k}^{(l)}\\subseteq\\tilde{\\mathcal{H}}_{kK^{l}}$ The base case for $k=1$ holds trivially. Now assume Eq. (38) holds for k, then",
    "607": "$(\\sum_{\\omega'\\in\\pi^{(l)}(\\Omega)}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j}))^{k+1}=(\\sum_{\\omega'\\in\\mathbb{R}^{(l)}(\\Omega)}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j}))^{k}(\\sum_{\\omega'\\in\\mathbb{R}^{(l)}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j}))$",
    "608": "(39)",
    "609": "$=(\\sum_{\\omega'\\in\\mathcal{H}_{k}^{(0)}\\tilde{b}_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\overline{\\phi}}_{\\omega',j}))(\\sum_{\\omega'\\in\\mathcal{H}^{(t)}\\langle\\Omega\\rangle}b_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\tilde{\\phi}_{\\omega',j}))$",
    "610": "$=\\sum_{\\omega'\\in\\mathcal{D}\\{\\mathcal{H}_{k}^{(t)},\\mathcal{H}^{(t)}\\}}\\tilde{\\overline{b}}\\omega',j}sin(\\langle\\omega',r\\rangle+\\overline{\\overline{\\phi}\\omega',j})$",
    "611": "where last equality holds because of Lemma 2 and we have:",
    "612": "$\\mathcal{D}\\{\\mathcal{H}_{k}^{(l)},\\mathcal{H}^{(l)}\\}=\\{\\omega_{1}\\pm\\omega_{2}|\\omega_{1}\\in\\mathcal{H}_{k}^{(l)},\\omega_{2}\\in\\mathcal{H}^{(l)}\\}$",
    "613": "$\\subseteq\\{\\omega_{1}\\pm\\omega_{2}|\\omega_{1}\\in\\overline{\\mathcal{H}}_{kK'},\\omega_{2}\\in\\overline{\\mathcal{H}}_{K'}\\}$",
    "614": "(40",
    "615": "(41)",
    "616": "(42)",
    "617": "(43)",
    "618": "$=\\{\\sum_{t=0}^{T-1}s_{t}^{(1)}\\Omega_{t}\\pm\\sum_{t=0}^{T-1}s_{t}^{(2)}\\Omega_{t}|\\sum_{t=0}^{T-1}|s_{t}^{(1)}|\\le kK^{l},\\sum_{t=0}^{T-1}|s_{t}^{(2)}|\\le K^{t}\\}$ (44)",
    "619": "$=\\{\\sum_{t=0}^{T-1}(s_{t}^{(1)}\\pm s_{t}^{(2)})\\Omega_{t}|\\sum_{t=0}^{T-1}|s_{t}^{(1)}|\\le kK^{l},\\sum_{t=0}^{T-1}|s_{t}^{(2)}|\\le K^{l}\\} \\subseteq\\{\\sum_{t=0}^{T-1}s_{t}'\\Omega_{t}|\\sum_{t=0}^{T-1}|s_{t}'|\\le(k+1)K^{l}\\}=\\tilde{\\mathcal{H}}_{(k+1)K^{l}}$",
    "620": "where the last line follows from triangle inequality. This proves our intermediate result in Eq. (38).",
    "621": "Now, let us use this result to complete the proof of the inductive step.",
    "622": "In particular, we can now write Eq. (37) as",
    "623": "$z_{j}^{(l+1)}=\\sum_{k=0}^{K}\\alpha_{k}\\sum_{\\omega'\\in\\mathcal{H}_{k}^{(l)}(\\Omega)}\\tilde{b}_{\\omega',j,k}sin(\\langle\\omega',r\\rangle+\\tilde{\\tilde{\\phi}}\\omega',j,k})$ $=\\sum_{\\omega'\\in\\mathcal{H}^{(l+1)}(\\Omega)}c_{\\omega',j}sin(\\langle\\omega',r\\rangle+\\phi\\omega',j})$",
    "624": "where $\\mathcal{H}^{(l+1)}:=\\bigcup_{k=1}^{K}\\mathcal{H}_{k}^{(l)}\\subseteq\\bigcup_{k=1}^{K}\\tilde{\\mathcal{H}}_{kK^{l}}\\subseteq\\tilde{\\mathcal{H}}_{KK^{l}}=\\tilde{\\mathcal{H}}_{K^{l+1}}.$ This sequence of inclusions concludes the proof.",
    "625": "(45)",
    "626": "(46)",
    "627": "(47)",
    "628": "(48)",
    "629": "(49)",
    "630": "16",
    "631": "Π"
  },
  "Page 17": {
    "632": "A.2. Two-layer SIREN example",
    "633": "Example. Let $f_{\\theta}$ be a three-layer SIREN defined as",
    "634": "$f_{\\theta}(r)={w^{(2)}}^{\\top}sin(W^{(1)}sin(\\Omega r))$ ,",
    "635": "where $r\\in\\mathbb{R}$, $\\Omega\\in\\mathbb{R}^{T}$ $W^{(1)}\in\\mathbb{R}^{F\\times T}$, and $w^{(2)}\in\\mathbb{R}^{F}$ The output of this network can equivalently be represented",
    "636": "where $\\omega_{t}^{\\top}\\in\\mathbb{R}^{D}$ denotes the tth row of $\\Omega$ ,",
    "637": "$f_{\\theta}(r)=\\sum_{m=0}^{F-1}\\sum_{s_{1},...,s_T=-\\infty}^{\\infty}c_{m,s_{1},...,s_{T}}sin((\\sum_{t=0}^{T-1}s_{t}\\omega_{t})r)$ $c_{m,s_{1},...,s_{T}}=(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)}))w_{m}^{(2)}$",
    "638": "and $J_{s}$ denotes the Bessel function of first kind of order s.",
    "639": "Proof. As we have discussed before, the first layer in SIREN plays the role of the frequency mapping, i.e.",
    "640": "(50)",
    "641": "(51)",
    "642": "(52)",
    "643": "$z^{(0)}=sin(W^{(0)}r)=sin(\\Omega r)$.",
    "644": "(53)",
    "645": "Hence the input of a node at the next layer is a linear combination of sinusoids at mapping frequencies.",
    "646": "The output of a node at second layer can be written as:",
    "647": "$z_{m}^{(1)}=sin(W_{m,:}^{(1)}sin(\\Omega r))$",
    "648": "$=sin(\\sum_{t=0}^{T-1}W_{m,t}^{(1)}sin(\\omega_{t}r)) =Im\\{exp(j(\\sum_{t=0}^{T-1}W_{m,t}^{(1)}sin(\\omega_{t}r)))\\} =Im\\{\\prod_{t=0}^{T-1}exp(jW_{m,t}^{(1)}sin(\\omega_{t}r))\\} =Im\\{\\prod_{t=0}^{T-1}\\sum_{s_{t}=-\\infty}^{\\infty}J_{s_{t}}(W_{m,t}^{(1)})exp(js_{t}\\omega_{t}r)\\} =Im\\{\\sum_{s_{0}=-\\infty}^{\\infty}\\cdot\\cdot\\cdot\\sum_{s_{T-1}=-\\infty}^{\\infty}\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)})exp(js_{t}\\omega_{t}r)\\}$",
    "649": "$=\\sum_{s_{1},...,s_{T}=-\\infty}^{\\infty}Im\\{\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)})exp(js_{t}\\omega_{t}r)\\} =\\sum_{s_{1},...,s_{T}=-\\infty}^{\\infty}Im\\{(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)}))exp(j\\sum_{t=0}^{T-1}s_{t}\\omega_{t}r)\\} =\\sum_{s_{1},...,s_{T}=-\\infty}^{\\infty}(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)}))Im\\{exp(j\\sum_{t=0}^{T-1}s_{t}\\omega_{t}r)\\}$",
    "650": "$=\\sum_{s_{1},...,s_{T}=-\\infty}^{\\infty}(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)}))sin(\\sum_{t=0}^{T-1}s_{t}\\omega_{t}r)$",
    "651": "(54)",
    "652": "(55)",
    "653": "(56)",
    "654": "(57)",
    "655": "(58)",
    "656": "(59)",
    "657": "(60)",
    "658": "(61)",
    "659": "(62)",
    "660": "(63)",
    "661": "where $J_{n}(\\cdot)$ represents the Bessel function of the first kind of order n and (58) follows from the Fourier series expansion of",
    "662": "$exp(j\\beta~sin(\\omega_{0}x))$:",
    "663": "$exp(j\\beta~sin(\\omega_{0}x))=\\sum_{n=-\\infty}^{\\infty}J_{n}(\\beta)exp(jn\\omega_{0}x)$ .",
    "664": "Note that we have omitted the bias terms to simplify the notation.",
    "665": "These biases only change the phase terms in the sinusoids of the sum.",
    "666": "(64)",
    "667": "17"
  },
  "Page 18": {
    "668": "Therefore, the output of the neural network can be written as:",
    "669": "$f_{\\theta}(r)={w^{(2)}}^{\\top}z^{(1)}=\\sum_{m=0}^{F-1}w_{m}^{(2)}z_{m}^{(1)}=\\sum_{m=0}^{F-1}w_{m}^{(2)}\\sum_{s_{1},...,sT=-\\infty}^{\\infty}(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)})sin(\\sum_{t=0}^{T-1}s_{t}\\omega_{t}r)$",
    "670": "(65)",
    "671": "$=\\sum_{m=0}^{F-1}\\sum_{s_{1},...,s_{T}=-\\infty}^{\\infty}w_{m}^{(2)}(\\prod_{t=0}^{T-1}J_{s_{t}}(W_{m,t}^{(1)})sin(\\sum_{t=0}^{T-1}s_{t}\\omega_{t}r)$ ( (66 ) ) Π",
    "672": "18"
  },
  "Page 19": {
    "673": "B. Imperfect recovery",
    "674": "B.1. Experimental details",
    "675": "For the experiment in Section 4.1, we train an FFN with four fully connected layers: three hidden layers with dimension 256 followed by an output layer of dimension $C=3$ All the networks presented in Fig.2 are trained for 2000 iterations with Adam optimizer [23].",
    "676": "The training image has the size of $512\\times512$ and we use all the available pixels during training.",
    "677": "B.2. Additional experiments",
    "678": "We further demonstrate the imperfect recovery phenomenon with INRs for different networks and configurations.",
    "679": "In Fig. 6, we present the results for SIREN [51], where the first layer of the form $z^{(0)}=sin(\\omega_{0}(W^{(0)}r+b^{(0)}))$ can be considered as the input mapping $\\gamma(r)$.",
    "680": "For a fair comparison with FFNs and to better illustrate the strong dependence of the learned representation on the chosen mapping, we do not perform any updates on the parameters of the initial layer, i.e., $W^{(0)}$ and $b^{(0)}$, during training.",
    "681": "The initialization of these parameters ensures a similar mapping to that of FFN presented in Figure 2 of the main text, i.e. two single frequency mappings with frequencies $f_{0}=1$ and $f_{0}=0.5$ followed by a rich mapping.",
    "682": "As for the rest of the architecture, we use the same number of layers and training strategy.",
    "683": "As you can see in Fig. 6, initializing the SIREN with this $\\gamma(r)$ results also in a set $\\mathcal{H}(\\Omega)\\subseteq\\{2\\pi k|k\\in\\mathbb{Z}\\}$, which leads to a very imperfect recovery with the reconstruction looking aliased in the spatial domain.",
    "684": "(a) SIREN $(\\omega_{0}=1)$ (b) SIREN $(\\omega_{0}=1)$ (c) SIREN ( $\\omega_{0}=30)$ (d) Ground Truth $W^{(0)}=I$ $W^{(0)}=0.5\\times I$ $W^{(0)}\\in\\mathbb{R}^{256\\times2}$",
    "685": "Figure 6. Top row: Image reconstruction with SIREN [51] for different configurations.",
    "686": "Middle row: Magnitude of the DFT of the reconstruction. Bottom row: The center crop of size $32\\times32$ from the magnitude of the DFT of the reconstruction.",
    "687": "Note, however, that in the original formulation of SIREN [51], the parameters of the initial layer are also updated during training.",
    "688": "To see the effect that this has in the reconstruction, we repeat the same experiment with a SIREN with trainable $W^{(0)}$ and $b^{(0)}$.",
    "689": "Interestingly, as illustrated in Fig. 7, the aliasing effect is mostly eliminated in this case.",
    "690": "This a result of having a dynamic input mapping, which enables spreading the energy over all spectral coefficients rather than only the even ones.",
    "691": "Nevertheless, similar to our observations for Fig.2, we see that the reconstruction for low values of $\\omega_{0}$ is also blurry with most of the energy concentrated in the low frequencies.",
    "692": "For the sake of completeness, we also investigate the effect of having learnable parameters in $\\gamma(\\tau)$ for FFNs.",
    "693": "Specifically, we repeat this same experiment for the single frequency mapping of the main text, $\\gamma(r) = [cos(2\\pi f r), sin(2\\pi f r)]$, where we initialize the frequency variable f as $f_{0}$. Note that f, now, is a trainable parameter. We denote this network as $SFM^{*}$. Similarly, we also train with an $FFN^{*}$ with trainable $\\Omega_{i,i}$ randomly initialized as explained in Sec.2. Figure 8 shows the results of these experiments with identical findings as in the main text and the SIRENs presented above.",
    "694": "10 The notation, e.g., SIREN*, denotes the corresponding INR involving input mapping with learnable parameters.",
    "695": "19"
  },
  "Page 20": {
    "696": "(a) SIREN* $(\\omega_{0}=1)$ (b) SIREN* $(\\omega_{0}=1)$ (c) SIREN* $(\\omega_{0}=30)$ (d) Ground Truth $W^{(0)}=I$",
    "697": "$W^{(0)}=0.5\\times I$ $W^{(0)}\\in\\mathbb{R}^{2\\overline{5}6\\times2}$",
    "698": "Figure 7. Top row: Image reconstruction with SIREN [51] for different configurations.",
    "699": "The notation indicates that the parameters of the initial layer are subject to gradient updates during training.",
    "700": "Middle row: Magnitude of the DFT of the reconstruction. Bottom row: The center crop of size $32\\times32$ from the magnitude of the DFT of the reconstruction.",
    "701": "(a) SFM $(f_{0}=1)$ (b) SFM $(f_{0}=0.5)$ (c) FFN* $(\\sigma=1)$",
    "702": "(d) Ground Truth.",
    "703": "Figure 8. Top row: Image reconstruction with FFN [56] for different configurations.",
    "704": "SFM stands for single frequency mapping as in Figure 2, i.e., $\\gamma(r,f)=[cos(2\\pi fr)$, $sin(2\\pi fr)]^{T}$ where the frequency variable is initialized as $f_{0}$ and subject to gradient updates.",
    "705": "In general, the notation * indicates learnable input mapping. Middle row: Magnitude of the DFT of the reconstruction.",
    "706": "Bottom row: The center crop of size $32\\times32$ from the magnitude of the DFT of the reconstruction.",
    "707": "20"
  },
  "Page 21": {
    "708": "C. Aliasing",
    "709": "C.1. Experimental details",
    "710": "For the experiment in Section 4.2, we train a SIREN with three fully connected layers: two hidden layers with dimension 128 followed by the output layer of dimension 1. All the networks presented in Fig.3 are trained for 2000 iterations using Adam [23] with a learning rate of $1\\times10^{-4}$.",
    "711": "For the training data, we generate a single frequency signal $g(r)=sin(2\\pi\\cdot23r)$ on 128 evenly spaced samples in the range [0, 1], i.e., sampled with a frequency of $f_{*}=128$ and test the learned representation $f_{\\theta}(r)$ with samples from the same signal with $f_{s}=256$",
    "712": "C.2. Additional experiments",
    "713": "For the sake of completeness, and to show that aliasing is prevalent across INR families, we repeat the same experiment using FFNs [56] initialized with different $\\sigma$.",
    "714": "Recall that $\\sigma$ determines the standard deviation of the the distibution of $\\Omega_{i,j}$ at initialization, i.e., $\\Omega \\sim$ $\\mathcal{N}(0,\\sigma^{2})$.",
    "715": "The results presented in Fig. 9 highlight that the same aliasing phenomenon observed for SIRENs in Fig.3 happens identically for FFNs.",
    "716": "Magnitude Spectrum (dB)",
    "717": "$f=-23$ $f=23$",
    "718": "0",
    "719": "Magnitude Spectrum (dB) $f=-23$ $f=23$",
    "720": "40f $f=-105$",
    "721": "$f=105$",
    "722": "3",
    "723": "20",
    "724": "-100-",
    "725": "0",
    "726": "-50",
    "727": "f (Hz)",
    "728": "0",
    "729": "50",
    "730": "-100",
    "731": "0",
    "732": "100",
    "733": "f (Hz)",
    "734": "Magnitude Spectrum (dB)",
    "735": "50-",
    "736": "$f=-23$ $f=23$",
    "737": "Magnitude Spectrum (dB) $f=-23$ $f=23$",
    "738": "$f_{\\theta}(r)$ $g(r)$",
    "739": "30-",
    "740": "0",
    "741": "0",
    "742": "Mmm",
    "743": "-50-",
    "744": "-50",
    "745": "www 0",
    "746": "f (Hz)",
    "747": "50",
    "748": "-30- -100",
    "749": "0",
    "750": "100 f (Hz)",
    "751": "Sampling frequency $f_{s}=128Hz$",
    "752": "Sampling frequency $f_{s}=256Hz$",
    "753": "Figure 9. Magnitude of the spectrum of $g(r)=sin(2\\pi\\cdot23r)$ and its FFN [56] reconstruction trained at $f_{s}=128~Hz.$ Top row shows $\\sigma=100$, and bottom row $\\sigma=3$ On the left the signals are sampled at $f_{s}=128~Hz$ and on the right at $f_{s}=256~Hz.$",
    "754": "21"
  },
  "Page 22": {
    "755": "D. NTK eigenfunctions as dictionary atoms\n D.1. Estimation of eigenfunctions of the NTK",
    "756": "As it is common in the kernel literature, in this work, we use the eigenvectors of the kernel Gram matrix to approximate the eigenfunctions of the NTK.",
    "757": "Specifically, unless stated otherwise, in all our experiments we compute the Gram matrix of the NTK at any $\\theta_{0}$ using as samples the coordinates of all the pixels of an image, laid out on a grid of fixed resolution $(64\\times64)$.",
    "758": "That is, we compute a Gram matrix of size $64^{2}\\times64^{2}$ To that end, we use the empirical kernel_fn function from the neural tangents library [38] which allows to compute this matrix using a batch implementation.",
    "759": "Note that this operation can be computationally intense, scaling quadratically with the number of samples, but also quadratically with the number of outputs.",
    "760": "For this reason, we decided to use INRs with a single output and work only with grayscale images.",
    "761": "The results, however, are easily extensible to the multi-output setting.",
    "762": "Once we have the Gram matrix, we can perform its eigendecomposition and use the resulting eigenvectors to approximate the values of the eigenfunctions $\\phi_{j}(r)$ at the pixel coordinates.",
    "763": "The inner products $\\langle\\phi_{j},g_{n}\\rangle$ are then easily approximated as",
    "764": "D.2. Training details",
    "765": "$\\langle\\phi_{j},g_{n}\\rangle\\approx\\sum_{i=1}^{64^{2}}\\phi_{j}(r_{i})g_{n}(r_{i}).$ (67)",
    "766": "",
    "767": "In Section 5.2, we compare the generalization performance of several SIRENs (four hidden layers with dimension 256 followed by the output layer of dimension 1) with different initialization strategies.",
    "768": "To that end, we train each of these networks to reconstruct 100 validation images from the CelebA dataset using half of the pixels of the images for training.",
    "769": "The training pixel locations are random, but we use the same across all validation images.",
    "770": "Generalization performance, is then tested using the remaining half of the pixels.",
    "771": "To be consistent with the empirical protocol proposed in [55], we use full batch Adam [23] with a learning rate of $10^{-4}$ for the randomly initialized weights and use full batch gradient descent with learning rate $10^{-2}$ for the meta learned weights, which they reported to be the optimal choice of optimizers and learning rates for each individual case.",
    "772": "Figure 10 shows the evolution of the average training and test curves for each of these networks.",
    "773": "As we can see, the networks with a better energy concentration in Fig. 4, are the networks that train faster, and reach the best test performances.",
    "774": "25.0",
    "775": "22.5",
    "776": "20.0",
    "777": "17.5",
    "778": "15.0",
    "779": "12.5",
    "780": "10.0",
    "781": "7,5",
    "782": "2",
    "783": "6 iterations",
    "784": "(a) Test",
    "785": "30",
    "786": "25",
    "787": "ReLU-MLP",
    "788": "SIREN-5",
    "789": "SIREN-10",
    "790": "PSNR",
    "791": "20",
    "792": "SIREN-20",
    "793": "SIREN-40",
    "794": "15",
    "795": "SIREN-60",
    "796": "SIREN-80",
    "797": "SIREN-100",
    "798": "10",
    "799": "SIREN-30-Meta",
    "800": "10",
    "801": "4",
    "802": "Iterations",
    "803": "(b) Training",
    "804": "ReLU-MLP",
    "805": "SIREN-5",
    "806": "SIREN-10",
    "807": "SIREN-20",
    "808": "SIREN-40",
    "809": "SIREN-60",
    "810": "SIREN-80",
    "811": "SIREN-100",
    "812": "SIREN-30-Meta",
    "813": "10",
    "814": "Figure 10. The evolution of the reconstruction performance (average PSNR) of different representations for the first 10 training iterations when trained on 100 validation images from CelebA dataset.",
    "815": "The numbers for different SIREN instances indicate the value of $\\omega_{0}$.",
    "816": "Meta indicates the learned initialization, whereas all the other networks are initialized randomly in accordance with the original implementation [51].",
    "817": "D.3. Experiments on additional networks",
    "818": "For completeness, we also provide the results of these experiments using FFNs (a sinusoidal mapping of size 256 followed by three hidden layers with dimension 256 followed by the output layer of dimension 1), instead of SIRENs in Fig. 11. Again, we observe that those networks which have an energy profile more concentrated on the largest eigenvalues perform much better.",
    "819": "E. Meta-learning experiment",
    "820": "E.1. Experimental details",
    "821": "Our meta-learning experiments consist of two phases: A first pre-training phase, in which we use MAML, to meta-learn a good initialization from 5,000 training images from CelebA using a learning rate of $10^{-5}$ as indicated in [55] and 5000 meta-iterations.",
    "822": "In",
    "823": "22"
  },
  "Page 23": {
    "824": "Energy concentration $\\epsilon(\\lambda)$",
    "825": "1.0-",
    "826": "0.8-",
    "827": "ReLU-MLP",
    "828": "9.13",
    "829": "FFN-01",
    "830": "12.24",
    "831": "0.6-",
    "832": "FFN-05",
    "833": "11.53",
    "834": "FFN-1",
    "835": "11.76",
    "836": "0.4-",
    "837": "FFN-5",
    "838": "10.31",
    "839": "FFN-10",
    "840": "10.23",
    "841": "0.2-",
    "842": "FFN-1 Meta 17.31",
    "843": "0.0",
    "844": "λ",
    "845": "$10^{0}$ $10^{-1}$",
    "846": "$10^{-2}$",
    "847": "$10^{-3}$ $10^{-4}$ $10^{-5}$",
    "848": "Figure 11. Average energy concentration of 100 validation images from CelebA on subspaces spanned by the eigenfunctions of the empirical NTK associated to eigenvalues greater than a given threshold.",
    "849": "Legend shows the average test PSNR after training to reconstruct those images from 50% randomly selected pixels for 3 iterations.",
    "850": "The value following FFN specifies the $\\sigma$ parameter of the given network.",
    "851": "our experiments, we use a SIREN with four hidden layers with dimension 256 followed by the output layer of dimension 1, randomly initialized prior to meta-learning using $\\omega_{0}=30$ After pretraining, we finetune the networks starting at the meta-learned weights using the training protocol described in Appendix D.2.",
    "852": "To estimate the eigenfunctions of the NTK at the meta-learned weights we use the experimental setting described in Appendix E.1.",
    "853": "E.2. Experiments with an additional meta-learning algorithm",
    "854": "We repeat the same experiments by replacing the meta-learning algorithm with Reptile.",
    "855": "Fig. 12 shows the resulting energy concentration plot and Fig. 16 shows the eigenfunctions of the NTK at the meta-learned weights using Reptile.",
    "856": "As we can see, the results agree completely with those found using MAML.",
    "857": "This suggests that the reshaping effect of meta-learning on the NTK is a general phenomenon, which might be induced using multiple algorithms.",
    "858": "10",
    "859": "0.8",
    "860": "0.6",
    "861": "0.4",
    "862": "02",
    "863": "0.0",
    "864": "$10^{0}$",
    "865": "$10^{-1}$",
    "866": "$10^{-2}$",
    "867": "$10^{-3}$",
    "868": "$10^{-4}$",
    "869": "ReLU-MLP",
    "870": "9.11",
    "871": "SIREN-5",
    "872": "13.57",
    "873": "SIREN-10",
    "874": "15.73",
    "875": "SIREN-20",
    "876": "17.90",
    "877": "SIREN-40",
    "878": "18.51",
    "879": "SIREN-60",
    "880": "16.93",
    "881": "SIREN-80",
    "882": "15.53",
    "883": "SIREN-100",
    "884": "13.96",
    "885": "Meta-Reptile 22.81",
    "886": "$10^{-3}$",
    "887": "Figure 12. Average energy concentration of 100 validation images from CelebA on subspaces spanned by the eigenfunctions of the empirical NTK associated to eigenvalues greater than a given threshold.",
    "888": "Legend shows the average test PSNR after training to reconstruct those images from 50% randomly selected pixels.",
    "889": "The meta-learned weights are computed using Reptile.",
    "890": "E.3. Experiments on additional networks",
    "891": "For completeness, we also provide the results of these experiments using an FFN (a sinusoidal mapping of size 256 followed by three hidden layers with dimension 256 followed by the output layer of dimension 1) instead of a SIREN.",
    "892": "Prior to meta-learning the input mapping is initialized using $\\sigma=1$ Fig. 11 shows these additional results, where we see that meta-learning does also improve the energy concentration of the validation images on the principal eigenspace of the NTK for the FFNs.",
    "893": "Performance does also improve significantly in this case.",
    "894": "23"
  },
  "Page 24": {
    "895": "E.4. Comparison with standard training",
    "896": "Finally, as a baseline, we provide a comparison between standard training and meta-learning, both in terms of the dynamics of the NTK, and fine-tuning performance.",
    "897": "Specifically, we repeated the same experimental pipeline described before, where instead of pretraining using MAML, we pretrain a SIREN using Adam to reconstruct one training image in CelebA.",
    "898": "Energy concentration $\\epsilon(\\lambda)$ 1.0-",
    "899": "0.8-",
    "900": "0.6",
    "901": "Random 18.53",
    "902": "5 iter",
    "903": "17.15",
    "904": "0.4-",
    "905": "50 iter",
    "906": "16.50",
    "907": "5000 iter 13.50",
    "908": "0.2-",
    "909": "Meta",
    "910": "23.06",
    "911": "0.0",
    "912": "$10^{0}$ $10^{-1}$",
    "913": "$10^{-2}$",
    "914": "$10^{-3}$",
    "915": "$10^{-4}$",
    "916": "$10^{-5}$",
    "917": "Figure 13. Average energy concentration of 100 validation images from CelebA on subspaces spanned by the eigenfunctions of the empirical NTK associated to eigenvalues greater than a given threshold.",
    "918": "Legend shows the average test PSNR after training to reconstruct those images from 50% randomly selected pixels.",
    "919": "The number of iterations specify those of pretraining on the single task.",
    "920": "As we can see in Fig. 13 and Fig. 16 has a signficant impact on the NTK.",
    "921": "As described before in [26], training a neural network to reconstruct a signal transforms the eigenfunctions of the final NTK such that they look like the target signals.",
    "922": "Surprisingly, though, we are seeing that for this particular set of tasks, fine-tuning from those initializations results in worse performance than starting from scratch, i.e.. there is a negative transfer between different tasks.",
    "923": "PSNR",
    "924": "25.0-",
    "925": "22.5-",
    "926": "30-",
    "927": "20.0-",
    "928": "25-",
    "929": "17.5-",
    "930": "random",
    "931": "PSNR",
    "932": "20",
    "933": "random",
    "934": "5 iter",
    "935": "5 iter",
    "936": "15.0",
    "937": "50 iter",
    "938": "15-",
    "939": "50 iter",
    "940": "12.5-",
    "941": "5000 iter",
    "942": "5000 iter",
    "943": "10.0",
    "944": "Meta",
    "945": "10",
    "946": "Meta",
    "947": "2",
    "948": "4 Iterations",
    "949": "6",
    "950": "8",
    "951": "10",
    "952": "2",
    "953": "4",
    "954": "6 Iterations",
    "955": "8",
    "956": "10",
    "957": "(a) Test",
    "958": "(b) Training",
    "959": "Figure 14. The evolution of the reconstruction performance (PSNR) of a SIREN, where the parameters of the network are (i) initialized randomly, (ii) pretrained on a single image for different number of training iterations, and (iii) meta-learned.",
    "960": "Note that three different realizations of the networks with pretrained weights on a single image are provided for each different number of training iterations.",
    "961": "We can understand this phenomenon using the signal dictionary analogy, which is summarized by Fig. 13. As we can see, the more we pretrain the networks to reconstruct a specific training image, the more the energy profile of the validation set shifts to the smallest eigenvalues of the pretrained kernel, i.e, the learned dictionary has a worse compression performance than the randomly initialized NTK.",
    "962": "As a result, as illustrated in Fig. 14, we see that the more pretrained networks take longer to converge when fine-tuned, and reach worse generalization performances.",
    "963": "Inspecting the eigenfunctions of these different networks, and interpreting them as dictionary atoms, gives us one last piece of intuition to understand the dynamics of meta-learning.",
    "964": "Indeed, as we can see in Fig. 16 the eigenfunctions of the standardly trained networks do really look like the target signals, having very specific crisp edges and textures, which might not directly appear on other images.",
    "965": "That is, the dictionaries of these networks are overfitted to their specific pretraining task.",
    "966": "On the other hand, meta-learning can leverage much more data to construct a dictionary whose atoms can be easily composed to create face images.",
    "967": "We believe that understanding the dynamics of this process will be a very fruitful avenue for future research.",
    "968": "24"
  },
  "Page 25": {
    "969": "F. Performance on NTK eigenfunctions",
    "970": "In this section we empirically demonstrate that it is easier for a network to learn the NTK eigenfunctions corresponding to larger eigenvalues.",
    "971": "Figure 15 shows the PSNR reached after training the network for 2000 iterations with the target set to the NTK eigenfunction at initialization with the corresponding index.",
    "972": "Even though the meta-learned weights are used in this experiment, this phenomenon applies to other networks and weights as well and is previously discussed in [41].",
    "973": "PSNR",
    "974": "40.01",
    "975": "39.5-",
    "976": "39.0-",
    "977": "38.5-",
    "978": "38.0-",
    "979": "37.5-",
    "980": "37.0",
    "981": "36.5",
    "982": "0",
    "983": "200 400 600 Eigenfunction index",
    "984": "800",
    "985": "Figure 15. PSNR performance of a SIREN trained on increasing eigenfunctions of its NTK at initialization.",
    "986": "25"
  },
  "Page 26": {
    "987": "Random",
    "988": "Initialization",
    "989": "(1)",
    "990": "Random",
    "991": "Initialization",
    "992": "(2)",
    "993": "Random",
    "994": "Initialization",
    "995": "(3)",
    "996": "5 iterations",
    "997": "(Image 1)",
    "998": "50 iterations",
    "999": "(Image 1)",
    "1000": "5000 iterations",
    "1001": "(Image 1)",
    "1002": "5 iterations (Image 2)",
    "1003": "50 iterations",
    "1004": "(Image 2)",
    "1005": "5000 iterations",
    "1006": "(Image 2)",
    "1007": "5 iterations (Image 3)",
    "1008": "50 iterations",
    "1009": "(Image 3)",
    "1010": "5000 iterations",
    "1011": "(Image 3)",
    "1012": "Learned",
    "1013": "Initialization",
    "1014": "Figure 16. First ten eigenfunctions corresponding to the largest eigenvalues of the empirical NTK of SIREN [51] at initialization for different cases.",
    "1015": "The first three rows represent different realizations of random initialization.",
    "1016": "Rows 4-5-6 illustrate the eigenfunctions of a SIREN after meta-learning on a randomly chosen (single) training image from the CelebA dataset [28] with corresponding number of meta-learning updates on learnable parameters [55].",
    "1017": "Rows 7-8-9 and 10-11-12 are the outcomes of the same experiment for different choice of the training image used for meta-learning.",
    "1018": "The bottom row depicts the eigenfunctions of the learned initialization when we use 5,000 training images from the CelebA dataset for meta-learning.",
    "1019": "26"
  }
}